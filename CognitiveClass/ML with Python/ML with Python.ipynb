{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Python - CognitiveClass.ai\n",
    "\n",
    "[Course Link](https://courses.cognitiveclass.ai/courses/course-v1:BigDataUniversity+ML0101EN+2016_T3/info)\n",
    "\n",
    "### Supervised vs Unsupervised Learning\n",
    "\n",
    "#### ML vs Stat Modelling:\n",
    "\n",
    "ML is an algorithm that can learn from data without being reliant on std programming practices\n",
    "\n",
    "Stat modeling is the formulization of rel bw vars in form of mathematical eqns\n",
    "\n",
    "It is subfield of maths\n",
    "\n",
    "ML is subfield of CS and AI\n",
    "\n",
    "Naming conventions:\n",
    "\n",
    "<img src = \"./data/img/diag1\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "Other diff bw ML and SM:\n",
    "\n",
    "<img src = \"./data/img/diag2\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "**Supervised and Unsupervised Learning**\n",
    "\n",
    "In SL, we have a set of training data, or labeled data in which we know the structure and outcome of it\n",
    "\n",
    "<img src = \"./data/img/diag3\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "We take this data and train a ML model, so that it can understand patterns in the data\n",
    "\n",
    "Once the model has been trained we can use it to predict out-of-sample data,or data in which the results are unknown\n",
    "\n",
    "<img src = \"./data/img/diag4\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "If we are given data which is unstructured, we can use USL to find patterns in that data\n",
    "\n",
    "**SL**:\n",
    "\n",
    "We \"teach\" a model, ie we load the model with knowledge so that it can predict future instances\n",
    "\n",
    "How do we \"teach\" a model?\n",
    "\n",
    "- We teach it by training it with some data from a **labeled** dataset\n",
    "\n",
    "eg: Iris dataset\n",
    "\n",
    "<img src = \"./data/img/diag5\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "There are 2 types of SL:\n",
    "\n",
    "1. Classification\n",
    "\n",
    "2. Regression\n",
    "\n",
    "**USL**:\n",
    "\n",
    "We **do not** supervise the model, but we let the model work on its own to discover info that may not be visible to the human eye\n",
    "\n",
    "UL uses ML algos that draw conclusions on **unlabeled data**\n",
    "\n",
    "** Difficulties of USL vs SL **:\n",
    "\n",
    "- USL has more difficult algos, since we know little to no info about the data, or the outcomes that are to be expected\n",
    "\n",
    "- With USL we are trying to find things such as Groups or Clusters, Density Estimations, and Dim Redn\n",
    "\n",
    "- USL has fewer tests and models to ensure that outcome of model is accurate\n",
    "    As such, USL creates a **less controllable** env as machine is creating outcomes for us\n",
    "    \n",
    "- SL deals with labeled data, USL with unlabeled data\n",
    "    \n",
    "Eg:\n",
    "\n",
    "<img src = \"./data/img/diag6\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "Here we can see an op of an algo applied to examining poisonous mushrooms \n",
    "\n",
    "This is SL as its a Classification prob\n",
    "\n",
    "Algo: Classification Tree\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SL - Classification:\n",
    "\n",
    "<img src = \"./data/img/diag7\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "Depending on data used to train model and type of modelused, there are virtually endless possible boundaries\n",
    "\n",
    "If we had a new out-of-sample datapoint:\n",
    "\n",
    "<img src = \"./data/img/diag8\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "The modelknows where to classify it\n",
    "\n",
    "**KNN**:\n",
    "\n",
    "<img src = \"./data/img/diag9\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "Here k = 5\n",
    "\n",
    "The black arrows connect the 5 nearerst pts to the unknown data pt (black cross)\n",
    "\n",
    "Here Xu is classified as red\n",
    "\n",
    "If k = 14:\n",
    "\n",
    "<img src = \"./data/img/diag10\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "Xu is classified as green\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USL\n",
    "\n",
    "USL usually deals with algos that utilize centroids.\n",
    "\n",
    "Some of these are:\n",
    "\n",
    "- K means Clustering\n",
    "- Hierarchial Clustering\n",
    "- Density based Clustering\n",
    "\n",
    "Real world eg: Classifying categories of music\n",
    "\n",
    "**Dimensionality Reduction ** and/or **Feature Extraction** play a crucial role in this by reducing redundant features to make classification easier\n",
    "\n",
    "USL can be used to create a Text-to-Speech program\n",
    "By looking at the amp, break and freq in the sound waves the machine can estimate what word has been spoken\n",
    "and convert it to text\n",
    "\n",
    "USL can also be used for Image Processing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab - ML Basics\n",
    "\n",
    "In this lab exercise, you will learn some basic functions for viewing and analysing data such as target, feature names, etc. Also, you will get a basic understanding of how to use data to fit (train) a model and use it to make a prediction. This will serve as a building block for future labs!\n",
    "\n",
    "### Hello! We will start by introducing you to the digits dataset.\n",
    "\n",
    "The digits dataset is made of up of 1797 8x8 images such as the one below.\n",
    "<img src = \"https://ibm.box.com/shared/static/psb68kpyyt0o6kbhcq88cwj7fuv7nlhq.png\">\n",
    "These images are hand-written digits converted into image format. <br>\n",
    "We can use this data to train our machine to further determine other 8x8 images as specific digits! <br>\n",
    "Sounds like we are <i>Classifying</i> data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will need to import the dataset from sklearn and declare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "[[  0.   0.   5. ...,   0.   0.   0.]\n",
      " [  0.   0.   0. ...,  10.   0.   0.]\n",
      " [  0.   0.   0. ...,  16.   9.   0.]\n",
      " ..., \n",
      " [  0.   0.   1. ...,   6.   0.   0.]\n",
      " [  0.   0.   2. ...,  12.   0.   0.]\n",
      " [  0.   0.  10. ...,  12.   1.   0.]]\n",
      "1797\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "# Now let's check out the type and data for digits. \n",
    "# The type should be 'Bunch' which is a dictionary-like object \n",
    "# specifically useful for loading sklearn internal sample datasets.\n",
    "\n",
    "print (type(digits))\n",
    "\n",
    "print (digits.data)\n",
    "\n",
    "print(len(digits.data))\n",
    "\n",
    "print (len(digits.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1797 samples\n",
    "\n",
    "Each: 64 features\n",
    "\n",
    "Let's check out the description of this dataset for more information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optical Recognition of Handwritten Digits Data Set\n",
      "===================================================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      "References\n",
      "----------\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ((digits.DESCR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the categories that classify each of the images by invoking the target field. There is a number associated to the classification of each digit. The target field fetches these numbers, where each digit is mapped to a name in target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ..., 8 9 8]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "1797\n"
     ]
    }
   ],
   "source": [
    "print (digits.target)\n",
    "\n",
    "# Now if we print out the target_names, we can find out what the data is categorized as.\n",
    "\n",
    "print(digits.target_names)\n",
    "\n",
    "# An important piece of information to note is that the data is stored as a numpy datatype, \n",
    "# which is a homogeneous multidimensional array (ndarray).\n",
    "\n",
    "print (type(digits.data))\n",
    "print (type(digits.target))\n",
    "print (type(digits.target_names))\n",
    "print(len(digits.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's confirm that the shape of the data and target match (first column) \n",
    "\n",
    "Note: The shape of the data is a tuple, where the first field is the number of observations and the second field is the number of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "print(digits.data.shape)\n",
    "\n",
    "print(digits.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can declare variables for the data and target which will be used to fit (train) the machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to <b>import svm</b> (an algorithm) and declare a variable called clf with gamma and C attributes.\n",
    "Now we can <b>fit</b> our model and <b>predict</b> the last digit, which should be 8. <br>\n",
    "<b>Note</b>: The predict function will show a warning when run. Please ignore the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predn of last element:\n",
      "8\n",
      "Actual value of last element\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100)\n",
    "clf.fit(X,y)\n",
    "\n",
    "print(\"Predn of last element:\")\n",
    "print(clf.predict(digits.data)[-1])\n",
    "print(\"Actual value of last element\")\n",
    "print (digits.target[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab - Skulls Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style type=\"text/css\">\n",
       "    #ans:hover { background-color: black; }\n",
       "    #ans {padding: 6px; \n",
       "        background-color: white; \n",
       "        border: green 2px solid; \n",
       "        font-weight: bold; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style type=\"text/css\">\n",
    "    #ans:hover { background-color: black; }\n",
    "    #ans {padding: 6px; \n",
    "        background-color: white; \n",
    "        border: green 2px solid; \n",
    "        font-weight: bold; }\n",
    "</style>\n",
    "\"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a little information about the dataset. We are using a dataset called skulls.csv, which contains the measurements made on Egyptian skulls from five epochs.\n",
    "\n",
    "<img src = \"https://ibm.box.com/shared/static/bjlklq4cwp22twi6756d2wqyn16gerh1.png\", position = \"center\">\n",
    "\n",
    "\n",
    "In the fields of chronology and periodization, an epoch is an instant in time chosen as the origin of a particular era. The \"epoch\" then serves as a reference point from which time is measured. Time measurement units are counted from the epoch so that the date and time of events can be specified unambiguously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The attributes of the data are as follows: \n",
    "\n",
    "<img src = \"https://ibm.box.com/shared/static/02z8krlr99hwrqa2ecx3ycuiwqkcuzjv.png\", align = 'left'>\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "<b>epoch</b> - The epoch corresponding to each skull. Assigned as a factor with levels c4000BC c3300BC, c1850BC, c200BC, and cAD150, where the years are only given approximately.\n",
    "\n",
    "<b>mb</b> - Maximal Breadth of the skull.\n",
    "\n",
    "<b>bh</b> - Basiregmatic Heights of the skull.\n",
    "\n",
    "<b>bl</b> - Basilveolar Length of the skull.\n",
    "\n",
    "<b>nh</b> - Nasal Heights of the skull.\n",
    "\n",
    "---\n",
    "\n",
    "#### Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 6)\n"
     ]
    }
   ],
   "source": [
    "# Save the \" skulls.csv \" data file into a variable called my_data\n",
    "\n",
    "my_data = pandas.read_csv('https://ibm.box.com/shared/static/u8orgfc65zmoo3i0gpt9l27un4o0cuvn.csv', delimiter=',')\n",
    "\n",
    "print (my_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>epoch</th>\n",
       "      <th>mb</th>\n",
       "      <th>bh</th>\n",
       "      <th>bl</th>\n",
       "      <th>nh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>c4000BC</td>\n",
       "      <td>131</td>\n",
       "      <td>138</td>\n",
       "      <td>89</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>c4000BC</td>\n",
       "      <td>125</td>\n",
       "      <td>131</td>\n",
       "      <td>92</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c4000BC</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "      <td>99</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>c4000BC</td>\n",
       "      <td>119</td>\n",
       "      <td>132</td>\n",
       "      <td>96</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>c4000BC</td>\n",
       "      <td>136</td>\n",
       "      <td>143</td>\n",
       "      <td>100</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    epoch   mb   bh   bl  nh\n",
       "0           1  c4000BC  131  138   89  49\n",
       "1           2  c4000BC  125  131   92  48\n",
       "2           3  c4000BC  131  132   99  50\n",
       "3           4  c4000BC  119  132   96  44\n",
       "4           5  c4000BC  136  143  100  54"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_data)\n",
    "\n",
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a model, the model requires two inputs, X and y\n",
    "<ul>\n",
    "    <li> X: Feature Matrix, or array that contains the data. </li>\n",
    "    <li> y: Response Vector, or 1-D array that contains the classification categories </li>\n",
    "</ul>\n",
    "\n",
    "<b> Note: We will not be able to use the built-in scikit-learn functions that was used with the digits dataset, since the data is not of type bunches. </b>\n",
    "\n",
    "------------\n",
    "There are some problems with the data in my_data:\n",
    "<ul>\n",
    "    <li> There is a header on the data (Unnamed: 0    epoch   mb   bh   bl  nh) </li>\n",
    "    <li> The data needs to be in numpy.ndarray format in order to use it in the machine learning model </li>\n",
    "    <li> There is non-numeric data within the dataset </li>\n",
    "    <li> There are row numbers associated with each row that affect the model </li>\n",
    "</ul>\n",
    "\n",
    "To resolve these problems, I have created a function that fixes these for us:\n",
    "<b> removeColumns(pandasArray, column) </b>\n",
    "\n",
    "This function produces one output and requires two inputs.\n",
    "<ul>\n",
    "    <li> 1st Input: A pandas array. The pandas array we have been using is my_data </li>\n",
    "    <li> 2nd Input: Any number of integer values (order doesn't matter) that represent the columns that we want to remove. (Look at the data again and find which column contains the non-numeric values). We also want to remove the first column because that only contains the row number, which is irrelevant to our analysis.</li>\n",
    "    <ul>\n",
    "        <li> Note: Remember that Python is zero-indexed, therefore the first column would be 0. </li>\n",
    "    </ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'epoch', 'mb', 'bh', 'bl', 'nh'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[131 138  89  49]\n",
      " [125 131  92  48]\n",
      " [131 132  99  50]\n",
      " [119 132  96  44]\n",
      " [136 143 100  54]\n",
      " [138 137  89  56]\n",
      " [139 130 108  48]\n",
      " [125 136  93  48]\n",
      " [131 134 102  51]\n",
      " [134 134  99  51]\n",
      " [129 138  95  50]\n",
      " [134 121  95  53]\n",
      " [126 129 109  51]\n",
      " [132 136 100  50]\n",
      " [141 140 100  51]\n",
      " [131 134  97  54]\n",
      " [135 137 103  50]\n",
      " [132 133  93  53]\n",
      " [139 136  96  50]\n",
      " [132 131 101  49]\n",
      " [126 133 102  51]\n",
      " [135 135 103  47]\n",
      " [134 124  93  53]\n",
      " [128 134 103  50]\n",
      " [130 130 104  49]\n",
      " [138 135 100  55]\n",
      " [128 132  93  53]\n",
      " [127 129 106  48]\n",
      " [131 136 114  54]\n",
      " [124 138 101  46]\n",
      " [124 138 101  48]\n",
      " [133 134  97  48]\n",
      " [138 134  98  45]\n",
      " [148 129 104  51]\n",
      " [126 124  95  45]\n",
      " [135 136  98  52]\n",
      " [132 145 100  54]\n",
      " [133 130 102  48]\n",
      " [131 134  96  50]\n",
      " [133 125  94  46]\n",
      " [133 136 103  53]\n",
      " [131 139  98  51]\n",
      " [131 136  99  56]\n",
      " [138 134  98  49]\n",
      " [130 136 104  53]\n",
      " [131 128  98  45]\n",
      " [138 129 107  53]\n",
      " [123 131 101  51]\n",
      " [130 129 105  47]\n",
      " [134 130  93  54]\n",
      " [137 136 106  49]\n",
      " [126 131 100  48]\n",
      " [135 136  97  52]\n",
      " [129 126  91  50]\n",
      " [134 139 101  49]\n",
      " [131 134  90  53]\n",
      " [132 130 104  50]\n",
      " [130 132  93  52]\n",
      " [135 132  98  54]\n",
      " [130 128 101  51]\n",
      " [137 141  96  52]\n",
      " [129 133  93  47]\n",
      " [132 138  87  48]\n",
      " [130 134 106  50]\n",
      " [134 134  96  45]\n",
      " [140 133  98  50]\n",
      " [138 138  95  47]\n",
      " [136 145  99  55]\n",
      " [136 131  92  46]\n",
      " [126 136  95  56]\n",
      " [137 129 100  53]\n",
      " [137 139  97  50]\n",
      " [136 126 101  50]\n",
      " [137 133  90  49]\n",
      " [129 142 104  47]\n",
      " [135 138 102  55]\n",
      " [129 135  92  50]\n",
      " [134 125  90  60]\n",
      " [138 134  96  51]\n",
      " [136 135  94  53]\n",
      " [132 130  91  52]\n",
      " [133 131 100  50]\n",
      " [138 137  94  51]\n",
      " [130 127  99  45]\n",
      " [136 133  91  49]\n",
      " [134 123  95  52]\n",
      " [136 137 101  54]\n",
      " [133 131  96  49]\n",
      " [138 133 100  55]\n",
      " [138 133  91  46]\n",
      " [137 134 107  54]\n",
      " [141 128  95  53]\n",
      " [141 130  87  49]\n",
      " [135 131  99  51]\n",
      " [133 120  91  46]\n",
      " [131 135  90  50]\n",
      " [140 137  94  60]\n",
      " [139 130  90  48]\n",
      " [140 134  90  51]\n",
      " [138 140 100  52]\n",
      " [132 133  90  53]\n",
      " [134 134  97  54]\n",
      " [135 135  99  50]\n",
      " [133 136  95  52]\n",
      " [136 130  99  55]\n",
      " [134 137  93  52]\n",
      " [131 141  99  55]\n",
      " [129 135  95  47]\n",
      " [136 128  93  54]\n",
      " [131 125  88  48]\n",
      " [139 130  94  53]\n",
      " [144 124  86  50]\n",
      " [141 131  97  53]\n",
      " [130 131  98  53]\n",
      " [133 128  92  51]\n",
      " [138 126  97  54]\n",
      " [131 142  95  53]\n",
      " [136 138  94  55]\n",
      " [132 136  92  52]\n",
      " [135 130 100  51]\n",
      " [137 123  91  50]\n",
      " [136 131  95  49]\n",
      " [128 126  91  57]\n",
      " [130 134  92  52]\n",
      " [138 127  86  47]\n",
      " [126 138 101  52]\n",
      " [136 138  97  58]\n",
      " [126 126  92  45]\n",
      " [132 132  99  55]\n",
      " [139 135  92  54]\n",
      " [143 120  95  51]\n",
      " [141 136 101  54]\n",
      " [135 135  95  56]\n",
      " [137 134  93  53]\n",
      " [142 135  96  52]\n",
      " [139 134  95  47]\n",
      " [138 125  99  51]\n",
      " [137 135  96  54]\n",
      " [133 125  92  50]\n",
      " [145 129  89  47]\n",
      " [138 136  92  46]\n",
      " [131 129  97  44]\n",
      " [143 126  88  54]\n",
      " [134 124  91  55]\n",
      " [132 127  97  52]\n",
      " [137 125  85  57]\n",
      " [129 128  81  52]\n",
      " [140 135 103  48]\n",
      " [147 129  87  48]\n",
      " [136 133  97  51]]\n"
     ]
    }
   ],
   "source": [
    "# Remove the column containing the target name since it doesn't contain numeric values.\n",
    "# Also remove the column that contains the row number\n",
    "# axis=1 means we are removing columns instead of rows.\n",
    "# Function takes in a pandas array and column numbers and returns a numpy array without\n",
    "# the stated columns\n",
    "\n",
    "\n",
    "def removeColumns(pandasArray, *column):\n",
    "    \n",
    "    # print ([column])\n",
    "    # print (pandasArray.values)\n",
    "    \n",
    "    return pandasArray.drop(pandasArray.columns[[column]], axis=1).values\n",
    "    \n",
    "new_data = removeColumns(my_data, 0,1)\n",
    "\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have one half of the required data to fit a model, which is X or new_data\n",
    "\n",
    "Next, we need to get the response vector y. Since we cannot use .target and .target_names, I have created a function that will do this for us.\n",
    "\n",
    "\n",
    "<b> targetAndtargetNames(numpyArray, targetColumnIndex) </b>\n",
    "\n",
    "This function produces two outputs, and requires two inputs.\n",
    "<ul>\n",
    "    <li> <font size = 3.5><b><i>1st Input</i></b></font>: A numpy array. The numpy array you will use is my_data.values (or my_data.as_matrix())</li>\n",
    "    <ul>\n",
    "        <li> Note: DO NOT USE <b> new_data </b> here. We need the original .csv data file without the headers </li>\n",
    "    </ul>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li> <font size = 3.5><b><i>2nd Input</i></b></font>: An integer value that represents the target column . (Look at the data again and find which column contains the non-numeric values. This is the target column)</li>\n",
    "    <ul>\n",
    "        <li> Note: Remember that Python is zero-indexed, therefore the first column would be 0. </li>\n",
    "   </ul>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "    <li> <font size = 3.5><b><i>1st Output</i></b></font>: The response vector (target) </li>\n",
    "    <li> <font size = 3.5><b><i>2nd Output</i></b></font>: The target names (target_names) </li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4]\n",
      "150\n",
      "[[1 'c4000BC' 131 138 89 49]\n",
      " [2 'c4000BC' 125 131 92 48]\n",
      " [3 'c4000BC' 131 132 99 50]\n",
      " [4 'c4000BC' 119 132 96 44]\n",
      " [5 'c4000BC' 136 143 100 54]\n",
      " [6 'c4000BC' 138 137 89 56]\n",
      " [7 'c4000BC' 139 130 108 48]\n",
      " [8 'c4000BC' 125 136 93 48]\n",
      " [9 'c4000BC' 131 134 102 51]\n",
      " [10 'c4000BC' 134 134 99 51]\n",
      " [11 'c4000BC' 129 138 95 50]\n",
      " [12 'c4000BC' 134 121 95 53]\n",
      " [13 'c4000BC' 126 129 109 51]\n",
      " [14 'c4000BC' 132 136 100 50]\n",
      " [15 'c4000BC' 141 140 100 51]\n",
      " [16 'c4000BC' 131 134 97 54]\n",
      " [17 'c4000BC' 135 137 103 50]\n",
      " [18 'c4000BC' 132 133 93 53]\n",
      " [19 'c4000BC' 139 136 96 50]\n",
      " [20 'c4000BC' 132 131 101 49]\n",
      " [21 'c4000BC' 126 133 102 51]\n",
      " [22 'c4000BC' 135 135 103 47]\n",
      " [23 'c4000BC' 134 124 93 53]\n",
      " [24 'c4000BC' 128 134 103 50]\n",
      " [25 'c4000BC' 130 130 104 49]\n",
      " [26 'c4000BC' 138 135 100 55]\n",
      " [27 'c4000BC' 128 132 93 53]\n",
      " [28 'c4000BC' 127 129 106 48]\n",
      " [29 'c4000BC' 131 136 114 54]\n",
      " [30 'c4000BC' 124 138 101 46]\n",
      " [31 'c3300BC' 124 138 101 48]\n",
      " [32 'c3300BC' 133 134 97 48]\n",
      " [33 'c3300BC' 138 134 98 45]\n",
      " [34 'c3300BC' 148 129 104 51]\n",
      " [35 'c3300BC' 126 124 95 45]\n",
      " [36 'c3300BC' 135 136 98 52]\n",
      " [37 'c3300BC' 132 145 100 54]\n",
      " [38 'c3300BC' 133 130 102 48]\n",
      " [39 'c3300BC' 131 134 96 50]\n",
      " [40 'c3300BC' 133 125 94 46]\n",
      " [41 'c3300BC' 133 136 103 53]\n",
      " [42 'c3300BC' 131 139 98 51]\n",
      " [43 'c3300BC' 131 136 99 56]\n",
      " [44 'c3300BC' 138 134 98 49]\n",
      " [45 'c3300BC' 130 136 104 53]\n",
      " [46 'c3300BC' 131 128 98 45]\n",
      " [47 'c3300BC' 138 129 107 53]\n",
      " [48 'c3300BC' 123 131 101 51]\n",
      " [49 'c3300BC' 130 129 105 47]\n",
      " [50 'c3300BC' 134 130 93 54]\n",
      " [51 'c3300BC' 137 136 106 49]\n",
      " [52 'c3300BC' 126 131 100 48]\n",
      " [53 'c3300BC' 135 136 97 52]\n",
      " [54 'c3300BC' 129 126 91 50]\n",
      " [55 'c3300BC' 134 139 101 49]\n",
      " [56 'c3300BC' 131 134 90 53]\n",
      " [57 'c3300BC' 132 130 104 50]\n",
      " [58 'c3300BC' 130 132 93 52]\n",
      " [59 'c3300BC' 135 132 98 54]\n",
      " [60 'c3300BC' 130 128 101 51]\n",
      " [61 'c1850BC' 137 141 96 52]\n",
      " [62 'c1850BC' 129 133 93 47]\n",
      " [63 'c1850BC' 132 138 87 48]\n",
      " [64 'c1850BC' 130 134 106 50]\n",
      " [65 'c1850BC' 134 134 96 45]\n",
      " [66 'c1850BC' 140 133 98 50]\n",
      " [67 'c1850BC' 138 138 95 47]\n",
      " [68 'c1850BC' 136 145 99 55]\n",
      " [69 'c1850BC' 136 131 92 46]\n",
      " [70 'c1850BC' 126 136 95 56]\n",
      " [71 'c1850BC' 137 129 100 53]\n",
      " [72 'c1850BC' 137 139 97 50]\n",
      " [73 'c1850BC' 136 126 101 50]\n",
      " [74 'c1850BC' 137 133 90 49]\n",
      " [75 'c1850BC' 129 142 104 47]\n",
      " [76 'c1850BC' 135 138 102 55]\n",
      " [77 'c1850BC' 129 135 92 50]\n",
      " [78 'c1850BC' 134 125 90 60]\n",
      " [79 'c1850BC' 138 134 96 51]\n",
      " [80 'c1850BC' 136 135 94 53]\n",
      " [81 'c1850BC' 132 130 91 52]\n",
      " [82 'c1850BC' 133 131 100 50]\n",
      " [83 'c1850BC' 138 137 94 51]\n",
      " [84 'c1850BC' 130 127 99 45]\n",
      " [85 'c1850BC' 136 133 91 49]\n",
      " [86 'c1850BC' 134 123 95 52]\n",
      " [87 'c1850BC' 136 137 101 54]\n",
      " [88 'c1850BC' 133 131 96 49]\n",
      " [89 'c1850BC' 138 133 100 55]\n",
      " [90 'c1850BC' 138 133 91 46]\n",
      " [91 'c200BC' 137 134 107 54]\n",
      " [92 'c200BC' 141 128 95 53]\n",
      " [93 'c200BC' 141 130 87 49]\n",
      " [94 'c200BC' 135 131 99 51]\n",
      " [95 'c200BC' 133 120 91 46]\n",
      " [96 'c200BC' 131 135 90 50]\n",
      " [97 'c200BC' 140 137 94 60]\n",
      " [98 'c200BC' 139 130 90 48]\n",
      " [99 'c200BC' 140 134 90 51]\n",
      " [100 'c200BC' 138 140 100 52]\n",
      " [101 'c200BC' 132 133 90 53]\n",
      " [102 'c200BC' 134 134 97 54]\n",
      " [103 'c200BC' 135 135 99 50]\n",
      " [104 'c200BC' 133 136 95 52]\n",
      " [105 'c200BC' 136 130 99 55]\n",
      " [106 'c200BC' 134 137 93 52]\n",
      " [107 'c200BC' 131 141 99 55]\n",
      " [108 'c200BC' 129 135 95 47]\n",
      " [109 'c200BC' 136 128 93 54]\n",
      " [110 'c200BC' 131 125 88 48]\n",
      " [111 'c200BC' 139 130 94 53]\n",
      " [112 'c200BC' 144 124 86 50]\n",
      " [113 'c200BC' 141 131 97 53]\n",
      " [114 'c200BC' 130 131 98 53]\n",
      " [115 'c200BC' 133 128 92 51]\n",
      " [116 'c200BC' 138 126 97 54]\n",
      " [117 'c200BC' 131 142 95 53]\n",
      " [118 'c200BC' 136 138 94 55]\n",
      " [119 'c200BC' 132 136 92 52]\n",
      " [120 'c200BC' 135 130 100 51]\n",
      " [121 'cAD150' 137 123 91 50]\n",
      " [122 'cAD150' 136 131 95 49]\n",
      " [123 'cAD150' 128 126 91 57]\n",
      " [124 'cAD150' 130 134 92 52]\n",
      " [125 'cAD150' 138 127 86 47]\n",
      " [126 'cAD150' 126 138 101 52]\n",
      " [127 'cAD150' 136 138 97 58]\n",
      " [128 'cAD150' 126 126 92 45]\n",
      " [129 'cAD150' 132 132 99 55]\n",
      " [130 'cAD150' 139 135 92 54]\n",
      " [131 'cAD150' 143 120 95 51]\n",
      " [132 'cAD150' 141 136 101 54]\n",
      " [133 'cAD150' 135 135 95 56]\n",
      " [134 'cAD150' 137 134 93 53]\n",
      " [135 'cAD150' 142 135 96 52]\n",
      " [136 'cAD150' 139 134 95 47]\n",
      " [137 'cAD150' 138 125 99 51]\n",
      " [138 'cAD150' 137 135 96 54]\n",
      " [139 'cAD150' 133 125 92 50]\n",
      " [140 'cAD150' 145 129 89 47]\n",
      " [141 'cAD150' 138 136 92 46]\n",
      " [142 'cAD150' 131 129 97 44]\n",
      " [143 'cAD150' 143 126 88 54]\n",
      " [144 'cAD150' 134 124 91 55]\n",
      " [145 'cAD150' 132 127 97 52]\n",
      " [146 'cAD150' 137 125 85 57]\n",
      " [147 'cAD150' 129 128 81 52]\n",
      " [148 'cAD150' 140 135 103 48]\n",
      " [149 'cAD150' 147 129 87 48]\n",
      " [150 'cAD150' 136 133 97 51]]\n",
      "['c4000BC', 'c3300BC', 'c1850BC', 'c200BC', 'cAD150']\n"
     ]
    }
   ],
   "source": [
    "def targetAndtargetNames(numpyArray, targetColumnIndex):\n",
    "    target_dict = dict()\n",
    "    target = list()\n",
    "    target_names = list()\n",
    "    count = -1\n",
    "    for i in range(len(my_data.values)):\n",
    "        if my_data.values[i][targetColumnIndex] not in target_dict:\n",
    "            count += 1\n",
    "            target_dict[my_data.values[i][targetColumnIndex]] = count\n",
    "        target.append(target_dict[my_data.values[i][targetColumnIndex]])\n",
    "    # Since a dictionary is not ordered, we need to order it and output it to a list so the\n",
    "    # target names will match the target.\n",
    "    for targetName in sorted(target_dict, key=target_dict.get):\n",
    "        target_names.append(targetName)\n",
    "    return np.asarray(target), target_names\n",
    "\n",
    "target, target_names = targetAndtargetNames(my_data, 1)\n",
    "\n",
    "print(target)\n",
    "\n",
    "print(len(target))\n",
    "\n",
    "print (my_data.values)\n",
    "\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "Now that we have the two required variables to fit the data, a sneak peak at how to fit data will be shown in the cell below.\n",
    "\n",
    "The data will be fit into a K-Nearest Neighbors model, which will be discussed more in a future lab.<br>\n",
    "<b>Note</b>: The predict function will show a warning when run. Please ignore the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4]\n",
      "Actual:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4]\n"
     ]
    }
   ],
   "source": [
    "X = new_data\n",
    "\n",
    "y = target\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "neigh.fit(X, y)\n",
    "\n",
    "print('Prediction:')\n",
    "\n",
    "print (neigh.predict(new_data))\n",
    "\n",
    "print('Actual:')\n",
    "\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "[2 0 0 0 2 3 1 0 0 0 3 4 0 0 0 3 0 1 2 0 0 0 0 0 1 2 0 0 1 0 0 0 2 1 1 1 1\n",
      " 1 0 1 0 3 1 2 0 1 1 0 1 3 1 0 3 1 0 3 1 0 3 0 2 0 3 0 2 2 2 3 2 0 3 2 2 2\n",
      " 1 0 1 4 2 4 1 0 2 1 2 4 0 1 2 2 1 3 3 2 3 3 4 2 2 2 3 1 1 3 3 3 1 0 3 3 3\n",
      " 4 2 0 1 3 3 4 3 1 4 2 0 1 3 0 4 1 3 4 4 0 4 4 2 1 2 4 1 4 2 1 4 0 3 4 1 1\n",
      " 4 1]\n",
      "Actual:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "neigh7 = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "neigh7.fit(X, y)\n",
    "\n",
    "print('Prediction:')\n",
    "\n",
    "print (neigh7.predict(new_data))\n",
    "\n",
    "print('Actual:')\n",
    "\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as you can see above, even though both <b>neigh</b> and <b>neigh7</b> were trained under the <b>same model</b>, by modifying the value of <b>n_neighbors</b> we obtained a <b>different prediction</b> for each.\n",
    "\n",
    "Therefore it's important to note the different components in an algorithm and understand how they affect your result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning I\n",
    "\n",
    "#### K-Nearest Neighbors\n",
    "\n",
    "KNN is a supervised learning method used for both **Classification and Regression**\n",
    "\n",
    "- for Classification, the op of KNN is the class of unknown data pt based on KNN of training data\n",
    "\n",
    "- for Regression, the op is the avg of the values of a target var based on KNN of training data\n",
    "\n",
    "In Classification, KNN works as follows:\n",
    "\n",
    "1. Pick a value of K\n",
    "\n",
    "2. Search for the K observations in the training data that are \"nearest\" to the measurements of the unknown data\n",
    "\n",
    "3. Predict the response of the unknown data point using the most popular response value from the K nearest neighbor\n",
    "\n",
    "** Very low value of K (=1)**\n",
    "\n",
    "We are trying to classify our data pts into 2 cats - Red and Blue\n",
    "\n",
    "<img src = \"./data/img/diag11\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "The black dotted ine is the curve that separates the blue and red classification regions\n",
    "\n",
    "We want to use a K that will create a separation of the regions that **best matches this black line**\n",
    "\n",
    "In this dig there are very jagged edges following the data pts\n",
    "\n",
    "We have an out-of-sample data pt (pink) \n",
    "\n",
    "Its nearest pt(k=1) is blue as thats the pt it is on\n",
    "\n",
    "This is a bad predn since more of the pts around it is red. So the pt should be considered red\n",
    "\n",
    "But since we predict it as blue, we can say we have captured the noise in the data, or we have chosen one of the pts that represents an anomaly in the data\n",
    "\n",
    "See the diag on the right\n",
    "\n",
    "The division line bw these 2 cats would be very **complex** \n",
    "\n",
    "A low K value causes a highly complex model which causes Overfitting of data\n",
    "\n",
    "Overfitting is bad as we want a general model that works for any data, not just the trained data\n",
    "\n",
    "** Very high value of K (=100)**\n",
    "\n",
    "Model becomes overly generalized\n",
    "\n",
    "This makes it diff to generalie bw 2 regions if a pt appears bw the 2 regs\n",
    "\n",
    "<img src = \"./data/img/diag12\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "White reg is a spot where neither blue or red can be decided\n",
    "\n",
    "Pink pt: out of sample data pt.But we cant tell which reg it belongs to\n",
    "\n",
    "If we see the black dotted line that we ideally want to follow, we see that the model does not follow this at all\n",
    "\n",
    "** Decent value (k=16)**\n",
    "\n",
    "The divide in the data **mimics the black dotted line **\n",
    "\n",
    "This produces a fairly accurate model for this dataset\n",
    "\n",
    "Also there are no areas of noise like in diag with a low k value\n",
    "\n",
    "Lighter shades simply mean that the model is less confident about the classification\n",
    "\n",
    "<img src = \"./data/img/diag13\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "Looking at the data the line would be something like the one at the right-top\n",
    "\n",
    "This is far more general than the one before (right-bottom)\n",
    "\n",
    "** KNN Regression **\n",
    "\n",
    "<img src = \"./data/img/diag14\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "This is used for predicting outcome of ind var given a set of ind vars\n",
    "\n",
    "A set of uncolored pts are drawn from set of y and x values\n",
    "\n",
    "These uncolored pts are training data\n",
    "\n",
    "We want to use KNN to predict y for x=5.6 (query point)\n",
    "\n",
    "Say k=1\n",
    "\n",
    "Here we search training set and locate the pt closest to query pt\n",
    "\n",
    "Here it is x = 6\n",
    "\n",
    "y(predicted) = y6 (y value for x=6)\n",
    "\n",
    "So for 1 nearest neighbor y = y6\n",
    "\n",
    "This point is the red circle in the diag\n",
    "\n",
    "Say k = 2\n",
    "\n",
    "We locate first 2 closest pts to x i.e x5 and x6\n",
    "\n",
    "y = (y5+y6)/2\n",
    "\n",
    "This is green circle\n",
    "\n",
    "Alternatively there is the weighted KNN Algo\n",
    "\n",
    "This assigns weights to the kn nearest neighbors depending on the distance to query pt\n",
    "\n",
    "Closer the neighboring pt is to the query pt, larger will be the wts\n",
    "\n",
    "Sum of wts == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "\n",
    "[Link for detailed explanation on DT and RF](https://github.com/ShaunakSen/Data-Science-Updated/blob/master/snippets_ML/DT%20and%20RF/DT%20and%20RF.ipynb)\n",
    "\n",
    "<img src = \"./data/img/diag15\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "DTs are built by splitting training set into distinct nodes\n",
    "\n",
    "One node contains all of or most of one category of the data\n",
    "\n",
    "These categories can be called subsets\n",
    "\n",
    "The diag is a wether classifier\n",
    "\n",
    "The decision to go out for a run is influenced by the weather forecast\n",
    "\n",
    "If Outlook is overcast, Yes\n",
    "\n",
    "If its sunny or raining we need more details\n",
    "\n",
    "The addnl forecast vars can be things sunch as humidity and wind\n",
    "\n",
    "Terminologies:\n",
    "\n",
    "- Node: A test for the values (data) of a certain attribute> The goal of a node is to split the dataset on an attr\n",
    "\n",
    "- Leaf: Terminal node that predicts the outcome\n",
    "\n",
    "- Root: The beginning node that contains the entire dataset\n",
    "\n",
    "- Entropy: The amt of information disorder or the amt of randomness in the data. The entropy in a node depends on how much random data is in that node\n",
    "\n",
    "- Information Gain: Info collected that can increase the level of certainty in a particular predn. Entropy and IG are kind of opposites.As randomness dec IG inc andvice versa.\n",
    "\n",
    "** Info Gain example - Good **\n",
    "\n",
    "<img src = \"./data/img/diag16\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "This is our intial data pts\n",
    "\n",
    "we will be using histograms to look at the chance that an out-of-sample data pt will be of certain color\n",
    "\n",
    "Initially there is 25 % chance of all colors\n",
    "\n",
    "1st split:\n",
    "\n",
    "<img src = \"./data/img/diag17\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "Such split produces most IG and greatly dec entropy\n",
    "\n",
    "2nd split\n",
    "\n",
    "<img src = \"./data/img/diag18\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "Most likely splits look like this\n",
    "\n",
    "This isnt perfect but right split has highest chance of being blue but not 100% as with the 1st split with red\n",
    "\n",
    "Say we want to plot out-of-sample pt\n",
    "\n",
    "We will want to decide what color this pt is\n",
    "\n",
    "If this pt is on LHS, we can most likely say this pt will be red\n",
    "\n",
    "Say a pt has the attr to move to RHS twice\n",
    "\n",
    "There is a high prob of being blue\n",
    "\n",
    "Also pt could be green as there is equal no of green in the split\n",
    "\n",
    "It could even be yellow but the chance is less as there was a higher chance of it going to left\n",
    "\n",
    "In 2nd split there is less IG than 1st split\n",
    "\n",
    "<img src = \"./data/img/diag19\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "\n",
    "There is a lot more entropy in data pts after 2nd split than in those after 1st, since there is more randomness\n",
    "\n",
    "We looked at an eg of IG that provided goog gain as well as a more realistic eg that still provided decent gain\n",
    "\n",
    "** Info Gain example - Bad **\n",
    "\n",
    "Same dataset\n",
    "\n",
    "<img src = \"./data/img/diag20\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "\n",
    "This split has 0 IG\n",
    "\n",
    "There is same prob of pt being R,G,B,Y as b4the split. So given an out-of-sample pt goes to RHS or LHS we can tell what color its likely to be\n",
    "\n",
    "No IG, so entropy remains same\n",
    "\n",
    "Its pointless\n",
    "\n",
    "\n",
    "#### DT: Adv and Disadv\n",
    "\n",
    "<img src = \"./data/img/diag21\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "We can calculate entropy and IG of DTs using formulas\n",
    "\n",
    "Small vars in data can completely change the tree: This is because the kinds of splits which are made can change\n",
    "\n",
    "Greedy Algo results in a faster runtime but might not produce an optimal tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "\n",
    "RF: many DTs\n",
    "\n",
    "Each DT is created from random splits in the dataset\n",
    "\n",
    "\n",
    "**RF Algo**\n",
    "\n",
    "1. For b=1 to B:\n",
    "\n",
    "    - Draw a bootstrap sample Z* of size N from the training data\n",
    "    \n",
    "    - Grow a RF tree Tb to the bootstrapped data, by recursively repeating the following steps for each terminal\n",
    "    node of the tree, until the minm node size n<sub>min</sub> is reached\n",
    "    \n",
    "        - Select m vars randomly from p vars\n",
    "        \n",
    "        - Pick best var/split pt among the m\n",
    "        \n",
    "        - Split the node into 2 daughter nodes\n",
    "    \n",
    "2. Output the ensemble of trees {Tb}<sub>1</sub><sup>B</sup>\n",
    "\n",
    "B: no of trees we wanna create\n",
    "\n",
    "Bootstrapping : ** u randomly select N data pts (with replacement) from a set of data**\n",
    "\n",
    "Some data pts may be chosen more than once and some may not be chosen at all\n",
    "\n",
    "2 sources of randomness:\n",
    "\n",
    "1. Randomness in the data\n",
    "\n",
    "2. Randomness in the Feature splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why RF when we have DTs already\n",
    "\n",
    "DTs are easy to build, use and interpret\n",
    "\n",
    "But in practice, theyare not that awesome\n",
    "\n",
    "> Trees have one aspect that prevents them from being the ideal tool for predictive learning, namely **inaccuracy**\n",
    "\n",
    "** They work great with the data used to create them but are not flexible when it comes to classifying\n",
    "new samples**\n",
    "\n",
    "RF combines simplicity of DTs with flexibility resulting in a vast improvement in accuracy\n",
    "\n",
    "Also another main reason is **Size of Dataset**:\n",
    "\n",
    "Say we have a v large dataset, with 1 mil obs and 10k attributes\n",
    "\n",
    "If we did binary splits on this huge dataset we would have to create an extremely large and complex tree\n",
    "\n",
    "<img src = \"./data/img/diag22\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "Such trees might take hrs or even days to build\n",
    "\n",
    "DTs dont work well with v large datasets as **depth of the tree becomes too large and it takes too long to classify a data pt**\n",
    "\n",
    "RF splits the datasets and creates multiple DTs\n",
    "\n",
    "These DTs can be built in **parallal increasing the pricessing speed**\n",
    "\n",
    "The data is split over many machines rather than storing all of the data in 1 machine\n",
    "\n",
    "Each mc can build a tree baised on the data that was stored on it\n",
    "\n",
    "Thus each tree will be build on a diff subset of the larger dataset\n",
    "\n",
    "We take avg of the trees to classify a new op\n",
    "\n",
    "This may not be true for other classifiers like KNN\n",
    "\n",
    "** Bagging : Bootstrapping the data plus using the aggregrate to make a decision is called Bagging **\n",
    "\n",
    "This allows RF to be a v good estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reliability of RF:\n",
    "\n",
    "RF splits the dataset randomly\n",
    "\n",
    "Say the data has been split into 4 subsets. 4 DTs can be built in parallel\n",
    "\n",
    "These smaller trees are built at the same time, so they dec the runtime\n",
    "\n",
    "While predicting outcome of a new pt, we traverse the pt through each tree until it reaches the leaf node\n",
    "\n",
    "Looking at where each pt is located in the respected tree, we can think of what color pt should be based on the other colors in that node\n",
    "\n",
    "<img src = \"./data/img/diag23\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "Based on all of these histograms we need to avg them to classify the unknown data pt\n",
    "\n",
    "We take the result of each color, add them up and div by the no of trees\n",
    "\n",
    "<img src = \"./data/img/diag24\" height=\"500\" width = \"500\" align=\"center\">\n",
    "\n",
    "Highest chance: Yellow\n",
    "\n",
    "Close 2nd : Green\n",
    "\n",
    "Low: Red or Blue\n",
    "\n",
    "Randomness in RF is of 2 types:\n",
    "\n",
    "1. Randomness in data\n",
    "\n",
    "2. Randomness in the split of the features\n",
    "\n",
    "Randomness is imp in RF as it allows us to have **distict diff trees that are based of diff data **\n",
    "\n",
    "DT that are v deep tend to overfit the data (low bias, high variance)\n",
    "\n",
    "When we have each tree based on diff subsets of the data, wach tree ends up having a lot higher variance\n",
    "\n",
    "By avg these trees we are able to red the overall var, although there is a small inc in bias\n",
    "\n",
    "Another reason why bootstrapping is good is bcoz we are able to make model better by preserving the bias, while dec the variance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab\n",
    "\n",
    "### K-Nearest Neighbors\n",
    "\n",
    "In this Lab you will load the Skulls dataset data, fit the data, and use K-Nearest Neighbors to predict a data point. But what is **K-Nearest Neighbors**?\n",
    "\n",
    "**K-Nearest Neighbors** is an algorithm for supervised learning. Where the data is 'trained' with data points corresponding to their classification. Once a point is to be predicted, it takes into account the 'K' nearest points to it to determine it's classification.\n",
    "\n",
    "#### Here's an visualization of the K-Nearest Neighbors algorithm.\n",
    "\n",
    "<img src = \"https://ibm.box.com/shared/static/mgkn92xck0z05v7yjq8pqziukxvc2461.png\">\n",
    "\n",
    "In this case, we have data points of Class A and B. We want to predict what the star (test data point) is. If we consider a k value of 3 (3 nearest data points) we will obtain a prediction of Class B. Yet if we consider a k value of 6, we will obtain a prediction of Class A.\n",
    "\n",
    "In this sense, it is important to consider the value of k. But hopefully from this diagram, you should get a sense of what the K-Nearest Neighbors algorithm is. It considers the 'K' Nearest Neighbors (points) when it predicts the classification of the test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style type=\"text/css\">\n",
       "    #ans:hover { background-color: black; }\n",
       "    #ans {padding: 6px; \n",
       "        background-color: white; \n",
       "        border: green 2px solid;\n",
       "        font-weight: bold}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style type=\"text/css\">\n",
    "    #ans:hover { background-color: black; }\n",
    "    #ans {padding: 6px; \n",
    "        background-color: white; \n",
    "        border: green 2px solid;\n",
    "        font-weight: bold}\n",
    "</style>\n",
    "\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using my_data as the <b>skulls.csv</b> data read by panda, declare variables <b>X</b> as the <b>Feature Matrix</b> (<i>data of my_data</i>) and <b>y</b> as the <b>response vector</b> (<i>target</i>)<br>\n",
    "<i>Note: Use the <b>target</b> function for the <b>response vector</b> and the <b>removeColumns</b> function for the <b>Feature Matrix</b> </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>epoch</th>\n",
       "      <th>mb</th>\n",
       "      <th>bh</th>\n",
       "      <th>bl</th>\n",
       "      <th>nh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>c4000BC</td>\n",
       "      <td>131</td>\n",
       "      <td>138</td>\n",
       "      <td>89</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>c4000BC</td>\n",
       "      <td>125</td>\n",
       "      <td>131</td>\n",
       "      <td>92</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c4000BC</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "      <td>99</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>c4000BC</td>\n",
       "      <td>119</td>\n",
       "      <td>132</td>\n",
       "      <td>96</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>c4000BC</td>\n",
       "      <td>136</td>\n",
       "      <td>143</td>\n",
       "      <td>100</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    epoch   mb   bh   bl  nh\n",
       "0           1  c4000BC  131  138   89  49\n",
       "1           2  c4000BC  125  131   92  48\n",
       "2           3  c4000BC  131  132   99  50\n",
       "3           4  c4000BC  119  132   96  44\n",
       "4           5  c4000BC  136  143  100  54"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/HSAUR/skulls.csv', delimiter=',')\n",
    "\n",
    "my_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(numpyArray, targetColumnIndex):\n",
    "    target_dict = dict()\n",
    "    target = list()\n",
    "    count = -1\n",
    "    for i in range(len(my_data.values)):\n",
    "        if my_data.values[i][targetColumnIndex] not in target_dict:\n",
    "            count += 1\n",
    "            target_dict[my_data.values[i][targetColumnIndex]] = count\n",
    "        target.append(target_dict[my_data.values[i][targetColumnIndex]])\n",
    "    return np.asarray(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the column containing the target name since it doesn't contain numeric values.\n",
    "# Also remove the column that contains the row number\n",
    "# axis=1 means we are removing columns instead of rows.\n",
    "# Function takes in a pandas array and column numbers and returns a numpy array without\n",
    "# the stated columns\n",
    "def removeColumns(pandasArray, *column):\n",
    "    return pandasArray.drop(pandasArray.columns[[column]], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 'c4000BC', 131, 138, 89, 49],\n",
       "       [2, 'c4000BC', 125, 131, 92, 48],\n",
       "       [3, 'c4000BC', 131, 132, 99, 50],\n",
       "       [4, 'c4000BC', 119, 132, 96, 44],\n",
       "       [5, 'c4000BC', 136, 143, 100, 54],\n",
       "       [6, 'c4000BC', 138, 137, 89, 56],\n",
       "       [7, 'c4000BC', 139, 130, 108, 48],\n",
       "       [8, 'c4000BC', 125, 136, 93, 48],\n",
       "       [9, 'c4000BC', 131, 134, 102, 51],\n",
       "       [10, 'c4000BC', 134, 134, 99, 51],\n",
       "       [11, 'c4000BC', 129, 138, 95, 50],\n",
       "       [12, 'c4000BC', 134, 121, 95, 53],\n",
       "       [13, 'c4000BC', 126, 129, 109, 51],\n",
       "       [14, 'c4000BC', 132, 136, 100, 50],\n",
       "       [15, 'c4000BC', 141, 140, 100, 51],\n",
       "       [16, 'c4000BC', 131, 134, 97, 54],\n",
       "       [17, 'c4000BC', 135, 137, 103, 50],\n",
       "       [18, 'c4000BC', 132, 133, 93, 53],\n",
       "       [19, 'c4000BC', 139, 136, 96, 50],\n",
       "       [20, 'c4000BC', 132, 131, 101, 49],\n",
       "       [21, 'c4000BC', 126, 133, 102, 51],\n",
       "       [22, 'c4000BC', 135, 135, 103, 47],\n",
       "       [23, 'c4000BC', 134, 124, 93, 53],\n",
       "       [24, 'c4000BC', 128, 134, 103, 50],\n",
       "       [25, 'c4000BC', 130, 130, 104, 49],\n",
       "       [26, 'c4000BC', 138, 135, 100, 55],\n",
       "       [27, 'c4000BC', 128, 132, 93, 53],\n",
       "       [28, 'c4000BC', 127, 129, 106, 48],\n",
       "       [29, 'c4000BC', 131, 136, 114, 54],\n",
       "       [30, 'c4000BC', 124, 138, 101, 46],\n",
       "       [31, 'c3300BC', 124, 138, 101, 48],\n",
       "       [32, 'c3300BC', 133, 134, 97, 48],\n",
       "       [33, 'c3300BC', 138, 134, 98, 45],\n",
       "       [34, 'c3300BC', 148, 129, 104, 51],\n",
       "       [35, 'c3300BC', 126, 124, 95, 45],\n",
       "       [36, 'c3300BC', 135, 136, 98, 52],\n",
       "       [37, 'c3300BC', 132, 145, 100, 54],\n",
       "       [38, 'c3300BC', 133, 130, 102, 48],\n",
       "       [39, 'c3300BC', 131, 134, 96, 50],\n",
       "       [40, 'c3300BC', 133, 125, 94, 46],\n",
       "       [41, 'c3300BC', 133, 136, 103, 53],\n",
       "       [42, 'c3300BC', 131, 139, 98, 51],\n",
       "       [43, 'c3300BC', 131, 136, 99, 56],\n",
       "       [44, 'c3300BC', 138, 134, 98, 49],\n",
       "       [45, 'c3300BC', 130, 136, 104, 53],\n",
       "       [46, 'c3300BC', 131, 128, 98, 45],\n",
       "       [47, 'c3300BC', 138, 129, 107, 53],\n",
       "       [48, 'c3300BC', 123, 131, 101, 51],\n",
       "       [49, 'c3300BC', 130, 129, 105, 47],\n",
       "       [50, 'c3300BC', 134, 130, 93, 54],\n",
       "       [51, 'c3300BC', 137, 136, 106, 49],\n",
       "       [52, 'c3300BC', 126, 131, 100, 48],\n",
       "       [53, 'c3300BC', 135, 136, 97, 52],\n",
       "       [54, 'c3300BC', 129, 126, 91, 50],\n",
       "       [55, 'c3300BC', 134, 139, 101, 49],\n",
       "       [56, 'c3300BC', 131, 134, 90, 53],\n",
       "       [57, 'c3300BC', 132, 130, 104, 50],\n",
       "       [58, 'c3300BC', 130, 132, 93, 52],\n",
       "       [59, 'c3300BC', 135, 132, 98, 54],\n",
       "       [60, 'c3300BC', 130, 128, 101, 51],\n",
       "       [61, 'c1850BC', 137, 141, 96, 52],\n",
       "       [62, 'c1850BC', 129, 133, 93, 47],\n",
       "       [63, 'c1850BC', 132, 138, 87, 48],\n",
       "       [64, 'c1850BC', 130, 134, 106, 50],\n",
       "       [65, 'c1850BC', 134, 134, 96, 45],\n",
       "       [66, 'c1850BC', 140, 133, 98, 50],\n",
       "       [67, 'c1850BC', 138, 138, 95, 47],\n",
       "       [68, 'c1850BC', 136, 145, 99, 55],\n",
       "       [69, 'c1850BC', 136, 131, 92, 46],\n",
       "       [70, 'c1850BC', 126, 136, 95, 56],\n",
       "       [71, 'c1850BC', 137, 129, 100, 53],\n",
       "       [72, 'c1850BC', 137, 139, 97, 50],\n",
       "       [73, 'c1850BC', 136, 126, 101, 50],\n",
       "       [74, 'c1850BC', 137, 133, 90, 49],\n",
       "       [75, 'c1850BC', 129, 142, 104, 47],\n",
       "       [76, 'c1850BC', 135, 138, 102, 55],\n",
       "       [77, 'c1850BC', 129, 135, 92, 50],\n",
       "       [78, 'c1850BC', 134, 125, 90, 60],\n",
       "       [79, 'c1850BC', 138, 134, 96, 51],\n",
       "       [80, 'c1850BC', 136, 135, 94, 53],\n",
       "       [81, 'c1850BC', 132, 130, 91, 52],\n",
       "       [82, 'c1850BC', 133, 131, 100, 50],\n",
       "       [83, 'c1850BC', 138, 137, 94, 51],\n",
       "       [84, 'c1850BC', 130, 127, 99, 45],\n",
       "       [85, 'c1850BC', 136, 133, 91, 49],\n",
       "       [86, 'c1850BC', 134, 123, 95, 52],\n",
       "       [87, 'c1850BC', 136, 137, 101, 54],\n",
       "       [88, 'c1850BC', 133, 131, 96, 49],\n",
       "       [89, 'c1850BC', 138, 133, 100, 55],\n",
       "       [90, 'c1850BC', 138, 133, 91, 46],\n",
       "       [91, 'c200BC', 137, 134, 107, 54],\n",
       "       [92, 'c200BC', 141, 128, 95, 53],\n",
       "       [93, 'c200BC', 141, 130, 87, 49],\n",
       "       [94, 'c200BC', 135, 131, 99, 51],\n",
       "       [95, 'c200BC', 133, 120, 91, 46],\n",
       "       [96, 'c200BC', 131, 135, 90, 50],\n",
       "       [97, 'c200BC', 140, 137, 94, 60],\n",
       "       [98, 'c200BC', 139, 130, 90, 48],\n",
       "       [99, 'c200BC', 140, 134, 90, 51],\n",
       "       [100, 'c200BC', 138, 140, 100, 52],\n",
       "       [101, 'c200BC', 132, 133, 90, 53],\n",
       "       [102, 'c200BC', 134, 134, 97, 54],\n",
       "       [103, 'c200BC', 135, 135, 99, 50],\n",
       "       [104, 'c200BC', 133, 136, 95, 52],\n",
       "       [105, 'c200BC', 136, 130, 99, 55],\n",
       "       [106, 'c200BC', 134, 137, 93, 52],\n",
       "       [107, 'c200BC', 131, 141, 99, 55],\n",
       "       [108, 'c200BC', 129, 135, 95, 47],\n",
       "       [109, 'c200BC', 136, 128, 93, 54],\n",
       "       [110, 'c200BC', 131, 125, 88, 48],\n",
       "       [111, 'c200BC', 139, 130, 94, 53],\n",
       "       [112, 'c200BC', 144, 124, 86, 50],\n",
       "       [113, 'c200BC', 141, 131, 97, 53],\n",
       "       [114, 'c200BC', 130, 131, 98, 53],\n",
       "       [115, 'c200BC', 133, 128, 92, 51],\n",
       "       [116, 'c200BC', 138, 126, 97, 54],\n",
       "       [117, 'c200BC', 131, 142, 95, 53],\n",
       "       [118, 'c200BC', 136, 138, 94, 55],\n",
       "       [119, 'c200BC', 132, 136, 92, 52],\n",
       "       [120, 'c200BC', 135, 130, 100, 51],\n",
       "       [121, 'cAD150', 137, 123, 91, 50],\n",
       "       [122, 'cAD150', 136, 131, 95, 49],\n",
       "       [123, 'cAD150', 128, 126, 91, 57],\n",
       "       [124, 'cAD150', 130, 134, 92, 52],\n",
       "       [125, 'cAD150', 138, 127, 86, 47],\n",
       "       [126, 'cAD150', 126, 138, 101, 52],\n",
       "       [127, 'cAD150', 136, 138, 97, 58],\n",
       "       [128, 'cAD150', 126, 126, 92, 45],\n",
       "       [129, 'cAD150', 132, 132, 99, 55],\n",
       "       [130, 'cAD150', 139, 135, 92, 54],\n",
       "       [131, 'cAD150', 143, 120, 95, 51],\n",
       "       [132, 'cAD150', 141, 136, 101, 54],\n",
       "       [133, 'cAD150', 135, 135, 95, 56],\n",
       "       [134, 'cAD150', 137, 134, 93, 53],\n",
       "       [135, 'cAD150', 142, 135, 96, 52],\n",
       "       [136, 'cAD150', 139, 134, 95, 47],\n",
       "       [137, 'cAD150', 138, 125, 99, 51],\n",
       "       [138, 'cAD150', 137, 135, 96, 54],\n",
       "       [139, 'cAD150', 133, 125, 92, 50],\n",
       "       [140, 'cAD150', 145, 129, 89, 47],\n",
       "       [141, 'cAD150', 138, 136, 92, 46],\n",
       "       [142, 'cAD150', 131, 129, 97, 44],\n",
       "       [143, 'cAD150', 143, 126, 88, 54],\n",
       "       [144, 'cAD150', 134, 124, 91, 55],\n",
       "       [145, 'cAD150', 132, 127, 97, 52],\n",
       "       [146, 'cAD150', 137, 125, 85, 57],\n",
       "       [147, 'cAD150', 129, 128, 81, 52],\n",
       "       [148, 'cAD150', 140, 135, 103, 48],\n",
       "       [149, 'cAD150', 147, 129, 87, 48],\n",
       "       [150, 'cAD150', 136, 133, 97, 51]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d391a656ee51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoveColumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-452254760e6e>\u001b[0m in \u001b[0;36mremoveColumns\u001b[0;34m(pandasArray, *column)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# the stated columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremoveColumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandasArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpandasArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandasArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2159\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3617\u001b[0m         \"\"\"\n\u001b[1;32m   3618\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_labels_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3619\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3620\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   2596\u001b[0m                                  'backfill or nearest reindexing')\n\u001b[1;32m   2597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2598\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ensure_platform_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.lookup\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "X = removeColumns(my_data, [0, 1])\n",
    "\n",
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
