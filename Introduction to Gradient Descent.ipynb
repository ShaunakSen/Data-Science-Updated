{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Introduction to Gradient Descent and Linear Regression\n",
    "\n",
    "References\n",
    "\n",
    "---\n",
    "\n",
    "[Siraj Raval video](https://www.youtube.com/watch?v=XdM6ER7zTLk)\n",
    "\n",
    "[Blog post 1](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)\n",
    "\n",
    "[Blog post 2](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/)\n",
    "\n",
    "Optimization is a big part of machine learning. Almost every machine learning algorithm has an optimization algorithm at it’s core.\n",
    "\n",
    "Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).\n",
    "\n",
    "Gradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuition for Gradient Descent\n",
    "\n",
    "Think of a large bowl like what you would eat cereal out of or store fruit in. This bowl is a plot of the cost function (f).\n",
    "\n",
    "![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/03/Large-Bowl.jpg)\n",
    "\n",
    "A random position on the surface of the bowl is the cost of the current values of the coefficients (cost).\n",
    "\n",
    "The bottom of the bowl is the cost of the **best set of coefficients, the minimum of the function.**\n",
    "\n",
    "The goal is to continue to try different values for the coefficients, evaluate their cost and select new coefficients that have a slightly better (lower) cost.\n",
    "\n",
    "Repeating this process enough times will lead to the bottom of the bowl and you will know the values of the coefficients that result in the minimum cost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Procedure\n",
    "\n",
    "The procedure starts off with initial values for the coefficient or coefficients for the function. These could be 0.0 or a small random value.\n",
    "\n",
    "    coefficient = 0.0\n",
    "\n",
    "The cost of the coefficients is evaluated by plugging them into the function and calculating the cost.\n",
    "\n",
    "    cost = f(coefficient)\n",
    "\n",
    "    or\n",
    "\n",
    "    cost = evaluate(f(coefficient))\n",
    "    \n",
    "The derivative of the cost is calculated. The derivative is a concept from calculus and refers to the slope of the function at a given point. We need to know the slope so that we know the direction (sign) to move the coefficient values in order to get a lower cost on the next iteration.\n",
    "\n",
    "    delta = derivative(cost)\n",
    "\n",
    "Now that we know from the derivative which direction is downhill, we can now update the coefficient values. A learning rate parameter (alpha) must be specified that controls how much the coefficients can change on each update.\n",
    "\n",
    "    coefficient = coefficient – (alpha * delta)\n",
    "\n",
    "This process is repeated until the cost of the coefficients (cost) is 0.0 or close enough to zero to be good enough.\n",
    "\n",
    "You can see how simple gradient descent is. It does require you to know the gradient of your cost function or the function you are optimizing, but besides that, it’s very straightforward. Next we will see how we can use this in machine learning algorithms.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
