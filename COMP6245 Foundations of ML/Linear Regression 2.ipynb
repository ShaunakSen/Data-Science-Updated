{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "%matplotlib inline \n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent(Amat, y, winit, rate, num_iter):\n",
    "    n, p = Amat.shape\n",
    "    w_history = []\n",
    "    loss_history = []\n",
    "    w = winit\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        loss = np.square(y - Amat.dot(w)).mean()\n",
    "        w_history.append(w)\n",
    "        loss_history.append(loss)\n",
    "        grad = (-2/n)*Amat.T.dot(y - Amat.dot(w))\n",
    "        w = w - rate * grad\n",
    "        print (\"Loss:\", loss)\n",
    "        print (\"Weights:\", w)\n",
    "        \n",
    "    return w, np.asarray(w_history), np.asarray(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.642097320953\n",
      "Weights: [-0.97569369  0.95081369 -0.15081611]\n",
      "Loss: 0.638773691602\n",
      "Weights: [-0.9741138   0.95153649 -0.15027695]\n",
      "Loss: 0.635469305709\n",
      "Weights: [-0.9725382   0.95225683 -0.14973973]\n",
      "Loss: 0.632184050436\n",
      "Weights: [-0.97096688  0.95297473 -0.14920444]\n",
      "Loss: 0.628917813605\n",
      "Weights: [-0.96939981  0.95369017 -0.14867107]\n",
      "Loss: 0.625670483698\n",
      "Weights: [-0.96783699  0.95440319 -0.14813961]\n",
      "Loss: 0.622441949854\n",
      "Weights: [-0.96627842  0.95511377 -0.14761007]\n",
      "Loss: 0.619232101859\n",
      "Weights: [-0.96472406  0.95582193 -0.14708244]\n",
      "Loss: 0.61604083015\n",
      "Weights: [-0.96317392  0.95652767 -0.1465567 ]\n",
      "Loss: 0.612868025804\n",
      "Weights: [-0.96162798  0.95723101 -0.14603286]\n",
      "Loss: 0.609713580542\n",
      "Weights: [-0.96008623  0.95793194 -0.14551091]\n",
      "Loss: 0.606577386716\n",
      "Weights: [-0.95854865  0.95863048 -0.14499084]\n",
      "Loss: 0.603459337313\n",
      "Weights: [-0.95701524  0.95932664 -0.14447265]\n",
      "Loss: 0.600359325949\n",
      "Weights: [-0.95548598  0.96002041 -0.14395633]\n",
      "Loss: 0.597277246863\n",
      "Weights: [-0.95396086  0.96071181 -0.14344188]\n",
      "Loss: 0.594212994916\n",
      "Weights: [-0.95243986  0.96140085 -0.14292928]\n",
      "Loss: 0.591166465586\n",
      "Weights: [-0.95092299  0.96208753 -0.14241855]\n",
      "Loss: 0.588137554966\n",
      "Weights: [-0.94941021  0.96277186 -0.14190966]\n",
      "Loss: 0.585126159757\n",
      "Weights: [-0.94790153  0.96345385 -0.14140261]\n",
      "Loss: 0.582132177269\n",
      "Weights: [-0.94639692  0.96413349 -0.14089741]\n",
      "Loss: 0.642097320953\n",
      "Weights: [-0.961436    0.95734111 -0.14594622]\n",
      "Loss: 0.609295279378\n",
      "Weights: [-0.94602407  0.96434717 -0.14072931]\n",
      "Loss: 0.578366993772\n",
      "Weights: [-0.93102956  0.97111396 -0.13570064]\n",
      "Loss: 0.549204051856\n",
      "Weights: [-0.91644033  0.97764863 -0.13085453]\n",
      "Loss: 0.521704323661\n",
      "Weights: [-0.90224457  0.98395811 -0.12618548]\n",
      "Loss: 0.495771597411\n",
      "Weights: [-0.88843085  0.99004912 -0.12168817]\n",
      "Loss: 0.471315236514\n",
      "Weights: [-0.87498803  0.99592821 -0.1173574 ]\n",
      "Loss: 0.448249856423\n",
      "Weights: [-0.86190535  1.00160171 -0.11318815]\n",
      "Loss: 0.426495020237\n",
      "Weights: [-0.84917232  1.00707577 -0.10917554]\n",
      "Loss: 0.405974951939\n",
      "Weights: [-0.83677879  1.01235637 -0.10531483]\n",
      "Loss: 0.386618266257\n",
      "Weights: [-0.82471489  1.01744929 -0.10160144]\n",
      "Loss: 0.368357714183\n",
      "Weights: [-0.81297105  1.02236018 -0.0980309 ]\n",
      "Loss: 0.351129943241\n",
      "Weights: [-0.80153797  1.02709448 -0.09459887]\n",
      "Loss: 0.334875271645\n",
      "Weights: [-0.79040664  1.03165749 -0.09130117]\n",
      "Loss: 0.319537475562\n",
      "Weights: [-0.77956829  1.03605435 -0.08813372]\n",
      "Loss: 0.305063588684\n",
      "Weights: [-0.76901444  1.04029006 -0.08509255]\n",
      "Loss: 0.291403713437\n",
      "Weights: [-0.75873683  1.04436945 -0.08217383]\n",
      "Loss: 0.278510843117\n",
      "Weights: [-0.74872746  1.04829723 -0.07937383]\n",
      "Loss: 0.266340694347\n",
      "Weights: [-0.73897855  1.05207796 -0.07668894]\n",
      "Loss: 0.254851549234\n",
      "Weights: [-0.72948257  1.05571606 -0.07411564]\n",
      "Loss: 0.642097320953\n",
      "Weights: [-0.89806849  0.98635189 -0.12430229]\n",
      "Loss: 0.487737156136\n",
      "Weights: [-0.82960776  1.01644954 -0.1020992 ]\n",
      "Loss: 0.374803934587\n",
      "Weights: [-0.77033061  1.04130165 -0.08401799]\n",
      "Loss: 0.292030841019\n",
      "Weights: [-0.71890183  1.06169291 -0.06943636]\n",
      "Loss: 0.231220514396\n",
      "Weights: [-0.6741823   1.07829239 -0.05782381]\n",
      "Loss: 0.186408644075\n",
      "Weights: [-0.63520016  1.09167051 -0.04872813]\n",
      "Loss: 0.153255486255\n",
      "Weights: [-0.60112623  1.10231361 -0.04176391]\n",
      "Loss: 0.128603192152\n",
      "Weights: [-0.57125301  1.11063626 -0.03660265]\n",
      "Loss: 0.110153764004\n",
      "Weights: [-0.54497679  1.11699186 -0.03296447]\n",
      "Loss: 0.0962347676827\n",
      "Weights: [-0.52178241  1.12168163 -0.03061088]\n",
      "Loss: 0.08562888814\n",
      "Weights: [-0.50123021  1.12496225 -0.02933872]\n",
      "Loss: 0.0774499304566\n",
      "Weights: [-0.48294497  1.12705249 -0.02897497]\n",
      "Loss: 0.0710526100262\n",
      "Weights: [-0.4666064   1.12813872 -0.0293723 ]\n",
      "Loss: 0.0659669243148\n",
      "Weights: [-0.4519411   1.1283797  -0.03040528]\n",
      "Loss: 0.0618504077068\n",
      "Weights: [-0.43871565  1.12791066 -0.03196717]\n",
      "Loss: 0.0584533962951\n",
      "Weights: [-0.42673072  1.12684675 -0.03396717]\n",
      "Loss: 0.0555937574113\n",
      "Weights: [-0.41581608  1.12528599 -0.03632803]\n",
      "Loss: 0.0531385047641\n",
      "Weights: [-0.40582634  1.1233118  -0.03898413]\n",
      "Loss: 0.0509904228682\n",
      "Weights: [-0.39663726  1.12099515 -0.0418797 ]\n",
      "Loss: 0.0490783357485\n",
      "Weights: [-0.38814267  1.11839642 -0.04496738]\n",
      "Loss: 0.642097320953\n",
      "Weights: [-0.81885909  1.02261536 -0.09724738]\n",
      "Loss: 0.357502098974\n",
      "Weights: [-0.70343499  1.07047904 -0.06254484]\n",
      "Loss: 0.213404643964\n",
      "Weights: [-0.61848477  1.10104162 -0.04141007]\n",
      "Loss: 0.139468654486\n",
      "Weights: [-0.55516596  1.11949606 -0.02972574]\n",
      "Loss: 0.100637937453\n",
      "Weights: [-0.50723268  1.12950427 -0.02459002]\n",
      "Loss: 0.0794354804872\n",
      "Weights: [-0.47027206  1.13364755 -0.02395888]\n",
      "Loss: 0.0671438554705\n",
      "Weights: [-0.44116522  1.13374458 -0.02639365]\n",
      "Loss: 0.0594107777034\n",
      "Weights: [-0.41770685  1.13107582 -0.03088282]\n",
      "Loss: 0.0540595165165\n",
      "Weights: [-0.39833658  1.12654195 -0.03671627]\n",
      "Loss: 0.0499978833609\n",
      "Weights: [-0.38194948  1.12077569 -0.04339646]\n",
      "Loss: 0.0466745925793\n",
      "Weights: [-0.36776215  1.1142207  -0.05057579]\n",
      "Loss: 0.0438083550785\n",
      "Weights: [-0.35521835  1.10718731 -0.05801237]\n",
      "Loss: 0.0412528645619\n",
      "Weights: [-0.34392222  1.09989183 -0.06553882]\n",
      "Loss: 0.0389294950013\n",
      "Weights: [-0.33359128  1.0924843  -0.07304021]\n",
      "Loss: 0.0367937376278\n",
      "Weights: [-0.32402317  1.08506807 -0.08043852]\n",
      "Loss: 0.0348184507689\n",
      "Weights: [-0.31507219  1.07771365 -0.08768171]\n",
      "Loss: 0.0329854886231\n",
      "Weights: [-0.30663273  1.07046841 -0.09473593]\n",
      "Loss: 0.031281506622\n",
      "Weights: [-0.29862757  1.06336351 -0.1015801 ]\n",
      "Loss: 0.0296958494216\n",
      "Weights: [-0.29099963  1.05641872 -0.10820203]\n",
      "Loss: 0.0282194780897\n",
      "Weights: [-0.28370616  1.04964586 -0.11459573]\n",
      "Loss: 0.642097320953\n",
      "Weights: [-0.18518394  1.31272312  0.11919196]\n",
      "Loss: 0.184125517237\n",
      "Weights: [-0.46795723  1.05877623 -0.0954412 ]\n",
      "Loss: 0.0713705264693\n",
      "Weights: [-0.26049634  1.10851893 -0.06531649]\n",
      "Loss: 0.0386891871284\n",
      "Weights: [-0.29666446  1.0264637  -0.13686557]\n",
      "Loss: 0.0261950778274\n",
      "Weights: [-0.22880594  1.01562233 -0.14942156]\n",
      "Loss: 0.0198379749229\n",
      "Weights: [-0.21867676  0.97884096 -0.18056404]\n",
      "Loss: 0.0159722420972\n",
      "Weights: [-0.18860764  0.96052003 -0.19539795]\n",
      "Loss: 0.0134278466085\n",
      "Weights: [-0.17388447  0.9386705  -0.21167999]\n",
      "Loss: 0.0116992644048\n",
      "Weights: [-0.15685394  0.92274108 -0.22215223]\n",
      "Loss: 0.0105070951234\n",
      "Weights: [-0.14496214  0.90754251 -0.23113676]\n",
      "Loss: 0.00967566702278\n",
      "Weights: [-0.13399365  0.89490517 -0.23734637]\n",
      "Loss: 0.00908873905644\n",
      "Weights: [-0.12534638  0.88346309 -0.24200264]\n",
      "Loss: 0.00866800866542\n",
      "Weights: [-0.11787924  0.87344324 -0.24503825]\n",
      "Loss: 0.00836041346085\n",
      "Weights: [-0.11173096  0.86439589 -0.24690329]\n",
      "Loss: 0.00812992404348\n",
      "Weights: [-0.10650789  0.85624998 -0.24773084]\n",
      "Loss: 0.00795206001535\n",
      "Weights: [-0.10212461  0.84880587 -0.24774258]\n",
      "Loss: 0.00781018379899\n",
      "Weights: [-0.09839815  0.84196532 -0.24706852]\n",
      "Loss: 0.00769298651167\n",
      "Weights: [-0.09523065  0.8356161  -0.24584062]\n",
      "Loss: 0.00759278038414\n",
      "Weights: [-0.0925165   0.82968038 -0.244156  ]\n",
      "Loss: 0.00750433851437\n",
      "Weights: [-0.09018037  0.82408703 -0.24210018]\n"
     ]
    }
   ],
   "source": [
    "# create small training data set ##########################\n",
    "def f(x):\n",
    "    return 0.5*(x)*(x**4)/(.05+(x**4))\n",
    "\n",
    "ntrain=5\n",
    "\n",
    "np.random.seed(0)\n",
    "X=np.linspace(0,1,ntrain)\n",
    "Xsq=np.square(X)\n",
    "\n",
    "Xsq\n",
    "\n",
    "y0=f(X)+0.03*np.random.normal(0,1,ntrain)\n",
    "\n",
    "y0\n",
    "\n",
    "Aquad=np.stack((np.ones(ntrain),X,Xsq)).T\n",
    "\n",
    "ntest = 50\n",
    "\n",
    "Xtest=np.linspace(0,1,ntest)\n",
    "\n",
    "n, p = Aquad.shape\n",
    "\n",
    "w0 = np.random.randn(p)\n",
    "\n",
    "rates = [.001,.01,.05,.1,.5]\n",
    "\n",
    "for i, r in enumerate(rates):\n",
    "    GradientDescent(Aquad, y0,  w0, r, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
