{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines - The Math of Intelligence (Week 1)\n",
    "\n",
    "[Video Link](https://www.youtube.com/watch?v=g8D5YL6cOSE&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D&t=0s&index=3)\n",
    "\n",
    "[GitHub Reference Link](https://github.com/llSourcell/Classifying_Data_Using_a_Support_Vector_Machine/blob/master/support_vector_machine_lesson.ipynb)\n",
    "\n",
    "[Contact](ss8n18@soton.ac.uk)\n",
    "---\n",
    "\n",
    "> We are going to build a SVM to classify 2 classes of data. We are going to optimize the SVM through Gradient Descent\n",
    "\n",
    "![](./data/img/diag1.png)\n",
    "\n",
    "We have 2 classes (red and blue dots)\n",
    "\n",
    "We plot both classes on a 2d plot\n",
    "\n",
    "We can draw a **decision boundary** that best separates these classes\n",
    "\n",
    "This boundary is called a **Hyperplane**. SVM helps us create this Hyperplane\n",
    "\n",
    "#### Use cases\n",
    "\n",
    "Claasification is the most popular use case for SVM\n",
    "\n",
    "But they can also be used for Regression, Outlier Detection, Clustering\n",
    "\n",
    "Here we are going to do **Supervised Classification** - our data has labels. We are trying to learn the **mapping bw the labels and the data**. If we learn the mapping i.e the function ( the function represents the mapping) the our ML has done its job. We can then use this function to plug in some ip data and get the predicted class as op\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does an SVM compare to other ML algorithms?\n",
    "\n",
    "- SVMs are great if we have **Small Datasets** (1000 rows or less)\n",
    "\n",
    "- Other models like Random Forests and Deep Neural Nw require much more data but come up wwith much more robust models\n",
    "\n",
    "![](https://camo.githubusercontent.com/3613584ae0fb8f1ec3f1c549a607a7969aa00d47/68747470733a2f2f696d6167652e736c696465736861726563646e2e636f6d2f6d736370726573656e746174696f6e2d3134303732323036353835322d70687061707030312f39352f6d73632d70726573656e746174696f6e2d62696f696e666f726d61746963732d372d3633382e6a70673f63623d31343036303132363130)\n",
    "\n",
    "Given 2 or more labeled classes of data, it acts as a discriminative classifier, formally defined by an optimal hyperplane that seperates all the classes. New examples that are then mapped into that same space can then be categorized based on on which side of the gap they fall.\n",
    "\n",
    "**The way we build this Hyperplane (the decision boundary bw the classes) is by maximizing the margin i.e the space bw that line and both of those classes**\n",
    "\n",
    "The points in each of the classes that are closest to the decision boundary are called **Support Vectors**. They are vectors(data pts) that support the creation of this Hyperplane\n",
    "\n",
    "![](https://camo.githubusercontent.com/ae3d247a4c7cf5bc9f4134a1a90c0df69b39e988/68747470733a2f2f7777772e64747265672e636f6d2f75706c6f616465642f70616765696d672f53766d4d617267696e322e6a7067)\n",
    "\n",
    "We are maximizing the magin - Why?\n",
    "\n",
    "- **This is because we want to draw a line ie in the perfect middle of the data, so that when we plot a new point, if its of a certain class, it will have max likelihood of falling on that side of the decision boundary where it should. The only way to maximize the space where a new data pt can fall into its correct class category is by maximizing the space bw the 2 classes and put a line right in the middle of that space**\n",
    "\n",
    "\n",
    "In the above diag, in 1st (left) case if we plot a new point it has less space to fall into the correct class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a Hyperplane\n",
    "\n",
    "![](https://camo.githubusercontent.com/6386085571d1ec30762907b62228ed414fd27335/687474703a2f2f736c696465706c617965722e636f6d2f736c6964652f313537393238312f352f696d616765732f33322f4879706572706c616e65732b61732b6465636973696f6e2b73757266616365732e6a7067)\n",
    "\n",
    "Geometry tells us that a hyperplane is a subspace of one dimension less than its ambient space. For instance, a hyperplane of an n-dimensional space is a flat subset with dimension n âˆ’ 1. By its nature, it separates the space into two half spaces.\n",
    "\n",
    "If we had a 400d spaced graph, then our Hyperplane would be 399d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear vs nonlinear classification\n",
    "\n",
    "We are only going to talk about linear classification\n",
    "\n",
    "![](./data/img/diag2.png)\n",
    "\n",
    "Say we have 2 classes of data and the decision boundary is not linear. So how do we build a Hyperplane?\n",
    "\n",
    "- we can do this via the **Kernel Trick**. We can map that ip space into a feature space st the Hyperplane we draw is linear\n",
    "\n",
    "So we are only talking about Supervised Linear Classification through SVM\n",
    "\n",
    "---\n",
    "\n",
    "No matter what model we are using, SVM, RF, DNN in the end we are guessing iteratively a **function**. This function represents the relationship bw all the vars in our data. If we find that function, then we have learned from our data.\n",
    "\n",
    "**Every ML model under the hood is just a function we are trying to approximate. It's coefficients are its weights. These wts are being updated over time through some Optimization techniques, be that Gradient Descent or Newton's Method etc.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Step 1 - Define our data\n",
    "\n",
    "# Input data form: [X value, Y value, Bias term]\n",
    "\n",
    "X = np.array([\n",
    "    [-2, 4, -1],\n",
    "    [4, 1, -1],\n",
    "    [1, 6, -1],\n",
    "    [2, 4, -1],\n",
    "    [6, 2, -1]\n",
    "])\n",
    "\n",
    "# Each of the Input data pts has an associated label, -1 or 1 \n",
    "\n",
    "y = np.array([-1,-1,1,1,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
