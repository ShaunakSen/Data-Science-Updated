{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines - The Math of Intelligence (Week 1)\n",
    "\n",
    "[Video Link](https://www.youtube.com/watch?v=g8D5YL6cOSE&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D&t=0s&index=3)\n",
    "\n",
    "[GitHub Reference Link](https://github.com/llSourcell/Classifying_Data_Using_a_Support_Vector_Machine/blob/master/support_vector_machine_lesson.ipynb)\n",
    "\n",
    "[Contact](ss8n18@soton.ac.uk)\n",
    "---\n",
    "\n",
    "> We are going to build a SVM to classify 2 classes of data. We are going to optimize the SVM through Gradient Descent\n",
    "\n",
    "![](./data/img/diag1.png)\n",
    "\n",
    "We have 2 classes (red and blue dots)\n",
    "\n",
    "We plot both classes on a 2d plot\n",
    "\n",
    "We can draw a **decision boundary** that best separates these classes\n",
    "\n",
    "This boundary is called a **Hyperplane**. SVM helps us create this Hyperplane\n",
    "\n",
    "#### Use cases\n",
    "\n",
    "Claasification is the most popular use case for SVM\n",
    "\n",
    "But they can also be used for Regression, Outlier Detection, Clustering\n",
    "\n",
    "Here we are going to do **Supervised Classification** - our data has labels. We are trying to learn the **mapping bw the labels and the data**. If we learn the mapping i.e the function ( the function represents the mapping) the our ML has done its job. We can then use this function to plug in some ip data and get the predicted class as op\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does an SVM compare to other ML algorithms?\n",
    "\n",
    "- SVMs are great if we have **Small Datasets** (1000 rows or less)\n",
    "\n",
    "- Other models like Random Forests and Deep Neural Nw require much more data but come up wwith much more robust models\n",
    "\n",
    "![](https://camo.githubusercontent.com/3613584ae0fb8f1ec3f1c549a607a7969aa00d47/68747470733a2f2f696d6167652e736c696465736861726563646e2e636f6d2f6d736370726573656e746174696f6e2d3134303732323036353835322d70687061707030312f39352f6d73632d70726573656e746174696f6e2d62696f696e666f726d61746963732d372d3633382e6a70673f63623d31343036303132363130)\n",
    "\n",
    "Given 2 or more labeled classes of data, it acts as a discriminative classifier, formally defined by an optimal hyperplane that seperates all the classes. New examples that are then mapped into that same space can then be categorized based on on which side of the gap they fall.\n",
    "\n",
    "**The way we build this Hyperplane (the decision boundary bw the classes) is by maximizing the margin i.e the space bw that line and both of those classes**\n",
    "\n",
    "The points in each of the classes that are closest to the decision boundary are called **Support Vectors**. They are vectors(data pts) that support the creation of this Hyperplane\n",
    "\n",
    "![](https://camo.githubusercontent.com/ae3d247a4c7cf5bc9f4134a1a90c0df69b39e988/68747470733a2f2f7777772e64747265672e636f6d2f75706c6f616465642f70616765696d672f53766d4d617267696e322e6a7067)\n",
    "\n",
    "We are maximizing the magin - Why?\n",
    "\n",
    "- This is because we want to draw a line ie in the perfect middle of the data, so that when we plot a new point, if its of a certain class, it will have max likelihood of falling on that side of the decision boundary where it should. The only way to maximize the space where a new data pt can fall into its correct class category is by maximizing the space bw the 2 classes and put a line right in the middle of that space\n",
    "\n",
    "\n",
    "In the above diag, in 1st (left) case if we plot a new point it has less space to fall into the correct class\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
