{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4: Neural Networks: Multilayer Perceptron Part 1 - The Nature of Code\n",
    "\n",
    "[Playlist link](https://www.youtube.com/watch?v=ntKn5TPHHAk&list=PLRqwX-V7Uu6Y7MdSCaIfsxc561QI0U0Tb&index=2)\n",
    "\n",
    "[README link](https://www.youtube.com/redirect?v=ntKn5TPHHAk&event=video_description&redir_token=-D8YTb5wSbqv2u0AQ2wgfaQYApl8MTUzNzEyNjY3MkAxNTM3MDQwMjcy&q=https%3A%2F%2Fgithub.com%2Fshiffman%2FNOC-S17-2-Intelligence-Learning%2Ftree%2Fmaster%2Fweek4-neural-networks)\n",
    "\n",
    "---\n",
    "\n",
    "#### Some recap\n",
    "\n",
    "We had a canvas full of pts and there was a line separating the 2 classes of pts. We had a simple perceptron trying to learn that line\n",
    "\n",
    "The scenario looked like this:\n",
    "\n",
    "![](./data/img/diag20.png)\n",
    "\n",
    "#### Why is this not enough?\n",
    "\n",
    "**What if we want any number of ips to generate any number of ops**. This is very common in many ML applications.\n",
    "Let us take a very classic classification problem\n",
    "\n",
    "Say we have a handwritten digit (say 8). I have all of the pixels (28x28) of that digit. These pixels (784 greyscale values) will be the ips to the Perceptron. The op will be a set of probabilities as to which digit it is\n",
    "\n",
    "![](./data/img/diag21.png)\n",
    "\n",
    "But we can simply have a whole bunch of ips, a whole bunch of ops but a single processing unit - Why cant we do that?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need of a MultiLayer Perceptron\n",
    "\n",
    "A single Perceptron can only solve **linearly separable problems**\n",
    "\n",
    "When we used a Perceptron in our prev example to classify the data we were dealing with a linearly separable prob. This means that it was possible to draw a st line that separated the 2 classes of data\n",
    "\n",
    "If it was in 3d I could put a plane and that would also be linearly separable because we can divide the space in half\n",
    "\n",
    "But most interesting probs are not LS\n",
    "\n",
    "![](./data/img/diag22.png)\n",
    "\n",
    "There is data in the canvas that clusters near the center. It is not LS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple use case for a Multilayer Perceptron\n",
    "\n",
    "Consider the Boolean expr A && B and see the truth table below:\n",
    "\n",
    "![](./data/img/diag23.png)\n",
    "\n",
    "\n",
    "This is a linearly seoparable prob as shown by the dotted line in the diagram\n",
    "\n",
    "This mean we can create a Perceptron with 2 ips which can be T or F and an op\n",
    "\n",
    "Same thing for A || B\n",
    "\n",
    "OR is also linearly separable\n",
    "\n",
    "So we can have Perceptrons learn to do A&&B and A||B\n",
    "\n",
    "![](./data/img/diag24.png)\n",
    "\n",
    "\n",
    "There is another bool expr called XOR. Its only true if one is T and one is F\n",
    "\n",
    "However it is not possible to draw a single line to divide the Ts and Fs\n",
    "\n",
    "![](./data/img/diag25.png)\n",
    "\n",
    "**So XOR is not a linearly separable prob**\n",
    "\n",
    "However we can draw **2 lines** to separate the Ts and Fs. That means a single simplest Perceptron cannot solve a simple operation like XOR.\n",
    "\n",
    "![](./data/img/diag26.png)\n",
    "\n",
    "---\n",
    "\n",
    "Say we have a perceptron that knows how to solve AND. If AND is linearly separable !AND ie **NAND** is also LS\n",
    "Also we have another perceptron that knows how to solve OR\n",
    "\n",
    "**More complex probs that are not linearly separable can be solved by liking multiple perceptrons together**\n",
    "\n",
    "![](./data/img/diag27.png)\n",
    "\n",
    "Suppose A and B are the ips \n",
    "\n",
    "The !AND or NAND computes $\\overline{AB}$\n",
    "\n",
    "The OR computes $A + B$\n",
    "\n",
    "The final AND does\n",
    "\n",
    "$\\overline{AB}\\cdot (A + B)$\n",
    "\n",
    "\n",
    "$\\overline{AB}\\cdot (A + B) = (\\overline{A} + \\overline{B})(A + B) = \\overline{A}B + A\\overline{B} = A \\bigoplus B $\n",
    "\n",
    "In the diag it is a 3 layered NN\n",
    "\n",
    "There is the ip layer, op layer and the hidden layer\n",
    "\n",
    "Hidden layer consists of the neurons that sit bw the ips and ops. They are called hidden because the user of the system does not see them. The user simply feeds in data and looks at the op. But the hidden layer is where the magic happens. This hidden layer is what allows us to get around the LS constraint. More hidden layers, more neurons, more complexity, more wts and params that need to be tweaked.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5: Neural Networks: Multilayer Perceptron Part 2 - The Nature of Code\n",
    "\n",
    "[GitHub link 1](https://github.com/CodingTrain/website/tree/master/Courses/natureofcode/10.18-toy_neural_network)\n",
    "[GitHub link 2](https://github.com/CodingTrain/Toy-Neural-Network-JS/)\n",
    "\n",
    "Here we want to design the very basic architecture of a NN library\n",
    "\n",
    "We want to create 3 things\n",
    "\n",
    "1. Input layer\n",
    "2. Hidden layer\n",
    "3. Output layer\n",
    "\n",
    "The way we want to design this library is:\n",
    "\n",
    "    new NeuralNetwork(num_ip_neurons, num_hidden_neurons, num_op_neurons)\n",
    "    \n",
    "Say there are going to be 3 ip neurons, 4 hidden neurons and 2 op neurons\n",
    "\n",
    "Say we consider the House Price Predn problem. The 3 ips will then be no of bedrooms, no of bathrooms, sq footage area\n",
    "\n",
    "We want a **Fully Connected Network**. This means that every ip is connected to every hidden neuron and every hidden neuron is connected to every op neuron\n",
    "\n",
    "So the 3 ips come in, the data feeds forward and the 2 ops come out\n",
    "\n",
    "![](./data/img/diag28.png)\n",
    "\n",
    "This is the overall structure of our **Multi Layered Feed Forward Fully Connected Perceptron**\n",
    "\n",
    "We want to first figure out in what shape our ips come in. Thats how many neurons we want for ip layer. What shape is the op that I want, that is how many op neurons I want. How many hidden neurons is kind of an open question\n",
    "\n",
    "Also this is an oversimplification of how NN architectures can be. This is by defn a 3 layered nw and this library is only going to allow for a 3 layered nw. But a lot of NN systems need multiple hidden layers as well\n",
    "\n",
    "Setting things up in code:\n",
    "\n",
    "``` javascript\n",
    "// constructor function\n",
    "\n",
    "function NeuralNetwork(numI, numH, numO){\n",
    "    this.input_nodes = numI;\n",
    "    this.hidden_nodes = numH;\n",
    "    this.output_nodes = numO;\n",
    "\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed Forward Process\n",
    "\n",
    "Lets pretend our NN is designed to solve the House pred prob\n",
    "\n",
    "The 3 ips will then be no of bedrooms (say 3), no of bathrooms (say 2), sq footage area (say 1000)\n",
    "\n",
    "![](./data/img/diag29.png)\n",
    "\n",
    "The number 3 comes in through the first ip neuron. Each connection bw the ips and the hidden neurons has a wt associated with it. Ultimately we want to tweak those wts to get good results (ops). \n",
    "\n",
    "Initially, these wts are going to have random values bw -1 and +1. \n",
    "\n",
    "Lets examone all the connections flowing in to the first hidden neuron. This particular neuron has 3 conn flowing in. Lets say the wts are 0.5, -0.5 and 1\n",
    "\n",
    "The hidden neuron computes a **weighted sum**\n",
    "\n",
    "Weighted sum for 1st hidden neuron \n",
    "\n",
    "    = 3 * 0.5 + 2 * (-0.5) + 1000 * 1\n",
    "\n",
    "    = 1000.5\n",
    "    \n",
    "The fact that sq footage is a large number weights the sum in its favour so we have to normalize the ips\n",
    "\n",
    "![](./data/img/diag30.png)\n",
    "\n",
    "Once the wt sum is computed, it gets passed through an **Activation Function** and then gets passed on to the ops\n",
    "\n",
    "#### Data Structure for storing the weighted connections\n",
    "\n",
    "We use a **Matrix**\n",
    "\n",
    "The idea here is to **store the weighted connections in a Matrix. The ips is an array. We take the ip array and mul it by the matrix of wts and generate the ops of the hidden layer**\n",
    "\n",
    "Lets look at a simpler diag with fewer neuron for understanding this\n",
    "\n",
    "![](./data/img/diag31.png)\n",
    "\n",
    "X1 and X2 are the 2 ips\n",
    "\n",
    "1 and 2 are the hidden layer neurons\n",
    "\n",
    "wij: weight bw input i and hidden neuron j\n",
    "\n",
    "The weight matrix:\n",
    "\n",
    "\\begin{bmatrix}w_{11} & w_{21} \\\\w_{12} & w_{22} \\end{bmatrix}\n",
    "\n",
    "The ip matrix:\n",
    "\n",
    "\\begin{bmatrix}x_{1} \\\\x_{2} \\end{bmatrix}\n",
    "\n",
    "Since this is one dimensional it is also called a **Vector**\n",
    "\n",
    "Now we need a wt sum that comes out of neuron 1. This wt sum is:\n",
    "\n",
    "$x_{1}\\times w_{11}\\times x_{2}\\times w_{21} \\rightarrow h_{1}$\n",
    "\n",
    "The wt sum of neuron 2 is:\n",
    "\n",
    "$x_{1}\\times w_{12}\\times x_{2}\\times w_{22} \\rightarrow h_{2}$\n",
    "\n",
    "Now we have:\n",
    "\n",
    "$\\begin{bmatrix}w_{11} & w_{21} \\\\w_{12} & w_{22} \\end {bmatrix}\\times\\begin{bmatrix}x_{1} \\\\x_{2} \\end{bmatrix}=\\begin{bmatrix}h_{1} \\\\h_{2} \\end{bmatrix}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Math - Linear Algebra + Coding the basic functions\n",
    "\n",
    "There are 2 key concepts in Linear Algebra:\n",
    "\n",
    "- Vector\n",
    "\n",
    "- Matrix\n",
    "\n",
    "Vector Dot Product:\n",
    "\n",
    "$\\begin{bmatrix}a_{x} & a_{y}  \\end {bmatrix}\\cdot\\begin{bmatrix}b_{x} \\\\b_{y} \\end{bmatrix}=a_{x}b_{x} + a_{y}b_{y}$\n",
    "\n",
    "Creating a Simple Matrix library:\n",
    "\n",
    "We basically want to have a constructor function which creates a matrix with specified no of rows and cols and initializes eaxh elem of the matrix to 0:\n",
    "\n",
    "``` javascript\n",
    "\n",
    "function Matrix(rows, cols){\n",
    "    this.rows = rows;\n",
    "    this.cols = cols;\n",
    "    this.matrix = [];\n",
    "\n",
    "    // loop through each row\n",
    "    for (var i = 0; i < this.rows; ++ i){\n",
    "        // every single row is also an array\n",
    "        this.matrix[i] = [];\n",
    "        for (var j = 0; j < this.cols; ++j){\n",
    "            // initialize each value to 0\n",
    "            this.matrix[i][j] = 0;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Coding the scalar functions:\n",
    "\n",
    "``` javascript\n",
    "\n",
    "// scalar addition function\n",
    "\n",
    "Matrix.prototype.add = function(n){\n",
    "    for(var i = 0; i < this.rows; ++i){\n",
    "        for(var j = 0; j < this.cols; ++j){\n",
    "            this.matrix[i][j] += n;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// scalar multiplication function\n",
    "\n",
    "Matrix.prototype.multiply = function(n){\n",
    "    for(var i = 0; i < this.rows; ++i){\n",
    "        for(var j = 0; j < this.cols; ++j){\n",
    "            this.matrix[i][j] *= n;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Randomly populate values in the matrix:\n",
    "\n",
    "``` javascript\n",
    "\n",
    "Matrix.prototype.randomize = function(){\n",
    "    for(var i = 0; i < this.rows; ++i){\n",
    "        for(var j = 0; j < this.cols; ++j){\n",
    "            this.matrix[i][j] = Math.floor(Math.random() * 10);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "**A really cool way to view Matrices in the console:**\n",
    "    \n",
    "    var m = new Matrix(3,2);\n",
    "    console.table(m.matrix);\n",
    "\n",
    "---\n",
    "\n",
    "Now we will look at **elementwise ops**\n",
    "\n",
    "While adding or subtracting 2 matrices we need to check that they have same dimensions\n",
    "\n",
    "We want to keep the same function add() which can receive scalars as well as matrices as ips and compute the result accordingly\n",
    "\n",
    "``` javascript\n",
    "\n",
    "Matrix.prototype.add = function(n){\n",
    "\n",
    "    // check if n is a matrix\n",
    "\n",
    "    if (n instanceof Matrix){\n",
    "        for(var i = 0; i < this.rows; ++i){\n",
    "            for(var j = 0; j < this.cols; ++j){\n",
    "                this.matrix[i][j] += n.matrix[i][j];\n",
    "            }\n",
    "        }\n",
    "    } else \n",
    "    // n is just a scalar number \n",
    "    {\n",
    "        for(var i = 0; i < this.rows; ++i){\n",
    "            for(var j = 0; j < this.cols; ++j){\n",
    "                this.matrix[i][j] += n;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Hadamard Product: elementwise product\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/06e3f6abf1511656029ce58b89695b687789aa9c)\n",
    "\n",
    "``` javascript\n",
    "\n",
    "multiply(n){\n",
    "    \n",
    "        if (n instanceof Matrix){\n",
    "            for(let i = 0; i < this.rows; ++i){\n",
    "                for(let j = 0; j < this.cols; ++j){\n",
    "                    this.matrix[i][j] *= n.matrix[i][j];\n",
    "                }\n",
    "            }\n",
    "        } else {\n",
    "            for(let i = 0; i < this.rows; ++i){\n",
    "                for(let j = 0; j < this.cols; ++j){\n",
    "                    this.matrix[i][j] *= n;\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    }\n",
    "\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
