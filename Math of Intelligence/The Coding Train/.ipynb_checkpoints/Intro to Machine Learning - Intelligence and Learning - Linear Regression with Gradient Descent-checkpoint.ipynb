{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 3 - Intro to Machine Learning - Intelligence and Learning\n",
    "\n",
    "[Playlist link](https://www.youtube.com/playlist?list=PLRqwX-V7Uu6bCN8LKrcMa6zF4FPtXyXYj)\n",
    "\n",
    "[Lesson README link](https://github.com/shiffman/NOC-S17-2-Intelligence-Learning/tree/master/week3-classification-regression)\n",
    "\n",
    "### 3.4: Linear Regression with Gradient Descent - Intelligence and Learning\n",
    "\n",
    "\n",
    "We already made a demo on LinReg with just 2 dimensions\n",
    "\n",
    "[Link to demo](link)\n",
    "\n",
    "But we might have a dataset with 100s of dimensions\n",
    "\n",
    "Here there is no easy statistical approach that can fit the data easily with a simple calculation\n",
    "\n",
    "This is the problem that deep learning (neural nw) based systems try to solve - to figure out a way to create a model to fit a given dataset\n",
    "\n",
    "One technique for doing that is **Gradient Descent**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does GD work\n",
    "\n",
    "Say we have a 2d space and a vehichle (triangle) that is moving around this space. It wants to reach the target (+).\n",
    "\n",
    "The vehichle has a paarticular velocity (expressed as the vector vel)\n",
    "\n",
    "The goal of the vehicle is to reach the target. In that case, its desired vel is as shown\n",
    "\n",
    "The current vel is like its guess. It starts going in that direction. But then it realizes it should go in another direction. It turns a **little bit** in that direction. It steers a little bit in that direction\n",
    "\n",
    "This is what GD does\n",
    "\n",
    "It feeds in a particular data pt (an x,y pair shown as the square in the left diag). It gets a guess (yguess) out of the ML recipe. \n",
    "    \n",
    "    error = y - yguess \n",
    "    \n",
    "Similarly in the vehicle eg\n",
    "\n",
    "    steer = desired - velocity\n",
    "    \n",
    "So if we use the error to tweak the parameters (in case of LinReg: m & b) of our model we will reduce error\n",
    "\n",
    "We do this over and over again\n",
    "\n",
    "This is **Supervised Learning**\n",
    "\n",
    "We start with random values of m and b.. It just plots a random line.. Then the line moves in the correct direction according to the error\n",
    "\n",
    "Note: In OLS method we used to square the error because we were only concerned with its magnitude\n",
    "Here we need the +ve or -ve value of the error to determine which way to tune our m & b\n",
    "\n",
    "\n",
    "Also, in the steering example there is a varibale called max_force. This denotes how powerful is my ability to turn.\n",
    "If max_force is too high we will **overshoot our target i.e region of minimum error**\n",
    "So we dont want to overshoot our optimal value\n",
    "\n",
    "This parmeter is called **Learning Rate**\n",
    "\n",
    "![](./data/img/diag7.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set initial guesses for m & b\n",
    "\n",
    "``` javascript\n",
    "\n",
    "var m = 0;\n",
    "var b = 0;\n",
    "\n",
    "```\n",
    "\n",
    "GradientDescent:\n",
    "\n",
    "``` javascript\n",
    "\n",
    "function gradientDescent(){\n",
    "\n",
    "    var learning_rate = 0.05;\n",
    "\n",
    "    for(var i = 0; i < data.length; ++i){\n",
    "        var x = data[i].x;\n",
    "        var y = data[i].y;\n",
    "\n",
    "        // make a guess\n",
    "        var guess = m * x + b;\n",
    "\n",
    "        //find error\n",
    "        var error = y - guess;\n",
    "\n",
    "        // tune m & b\n",
    "        m = m + (error * x) * learning_rate;\n",
    "        b = b + (error * learning_rate);\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, we should adjust the learning rate over time.\n",
    "\n",
    "One strategy is **Annealing**. We start with a high LR and then reduce it slowly over time. This is to get some big corrections in the beginning and then get some smaller corrections\n",
    "\n",
    "Also what we have used here is **Stochastic Gradient Descent** where we adjust the weights/parameters with every data pt.\n",
    "\n",
    "But we could also look at the error in totality the adjust the wts all at once at the end of one cycle through the data. - This is known as **Batch GD**\n",
    "\n",
    "[Interactive Code Demo](Codepen link)\n",
    "\n",
    "How did we get this?\n",
    "\n",
    "``` javascript\n",
    "m = m + (error * x) * learning_rate;\n",
    "b = b + (error * learning_rate);\n",
    "```\n",
    "How did we know exactly how to change m & b to minimize the error\n",
    "\n",
    "[We explore this here](link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
