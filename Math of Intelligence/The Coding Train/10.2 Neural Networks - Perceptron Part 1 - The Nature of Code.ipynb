{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2: Neural Networks: Perceptron Part 1 - The Nature of Code\n",
    "\n",
    "[Playlist link](https://www.youtube.com/watch?v=ntKn5TPHHAk&list=PLRqwX-V7Uu6Y7MdSCaIfsxc561QI0U0Tb&index=2)\n",
    "\n",
    "[README link](https://www.youtube.com/redirect?v=ntKn5TPHHAk&event=video_description&redir_token=-D8YTb5wSbqv2u0AQ2wgfaQYApl8MTUzNzEyNjY3MkAxNTM3MDQwMjcy&q=https%3A%2F%2Fgithub.com%2Fshiffman%2FNOC-S17-2-Intelligence-Learning%2Ftree%2Fmaster%2Fweek4-neural-networks)\n",
    "\n",
    "#### What is a Perceptron\n",
    "\n",
    "Perceptron is a model of a single neuron,the simplest possible ANN that we can build\n",
    "\n",
    "![](https://natureofcode.com/book/imgs/chapter10/ch10_03.png)\n",
    "\n",
    "There is a single neuron. It receives 2 ips (X0 and X1). These ips come into the neuron. Some type of mathematical process happens in the neuron and then there is an op (Y)\n",
    "\n",
    "This neuron /preceptron is actually like the ML recipe which receives ip and gives some op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand what exactly is happenning inside the NN we need to come up with some sort of a scenario.\n",
    "\n",
    "Say I have a 2D space. This space is divided by a line. Some pts will be on one side of the line (class A). Other pts will be on the other side (class B)\n",
    "\n",
    "We want to use the Perceptron for a **Classification** prob. For any pt in our 2d dataset it has a coordinate (x, y)\n",
    "\n",
    "Here x is input 0 (X0) to our perceeptron and y is input 1 (X1). The op is Class A or Class B.\n",
    "\n",
    "The Perceptron will op +1 (if class A) or -1 (if class B)\n",
    "\n",
    "![](./data/img/diag9.png)\n",
    "\n",
    "We are going to use Supervised Learning for this. We are going to give the perceptron a pt we already know the class of. Say we give it a pt of Class A. Perceptron guesses the class. If A, all good.. we just repeat the training process\n",
    "If B, we tweak the algo to try and get the correct answer. This tweaking is a process called Gradient Descent. \n",
    "\n",
    "#### What is the algo that runs inside the neuron\n",
    "\n",
    "Ips as they flow into the neuron are **weighted**.\n",
    "Each one of the connections has a wt\n",
    "\n",
    "wt for X0 = w0\n",
    "wt for X1 = w1\n",
    "\n",
    "Algo: create a sum of all ips multiplied by the wts(weighted sum of all the ips)\n",
    "\n",
    "Step 1.\n",
    "\n",
    "Sum = X0.w0 + X1.w1\n",
    "\n",
    "Step 2.\n",
    "\n",
    "**Activation Function**: It allows us to conform the op to some desired range. There are many diff AFs\n",
    "\n",
    "Here we will use a very simple AF. We want only 2 ops: +1 or -1. So we have to take the Sum and transform it\n",
    "\n",
    "SIGN(n) = +1 if n >= 0\n",
    "-1 if n < 0\n",
    "\n",
    "---\n",
    "\n",
    "This entire process is called **Feed Forward**: The ips come in, they get mul by the wts, get added, the weighted sum gets passed thru an activation function, we get +1 or -1 as op\n",
    "\n",
    "#### How to pick the wts\n",
    "\n",
    "The idea is that through the SL process we search for the optimal wt values that will give us results with least error.\n",
    "\n",
    "We can start off by taking random values of wts. **However there are many other methods for choosing initial wt values.**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
