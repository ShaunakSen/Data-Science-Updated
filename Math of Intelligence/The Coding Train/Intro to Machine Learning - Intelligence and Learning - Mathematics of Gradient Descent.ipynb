{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 3 - Intro to Machine Learning - Intelligence and Learning\n",
    "\n",
    "[Playlist link](https://www.youtube.com/playlist?list=PLRqwX-V7Uu6bCN8LKrcMa6zF4FPtXyXYj)\n",
    "\n",
    "[Lesson README link](https://github.com/shiffman/NOC-S17-2-Intelligence-Learning/tree/master/week3-classification-regression)\n",
    "\n",
    "### 3.5: Mathematics of Gradient Descent - Intelligence and Learning\n",
    "\n",
    "[YouTube link](https://www.youtube.com/watch?v=jc2IthslyzM&list=PLRqwX-V7Uu6bCN8LKrcMa6zF4FPtXyXYj&index=8)\n",
    "\n",
    "We know from previous lessons that\n",
    "\n",
    "error = guess - y\n",
    "\n",
    "The cost function here is :\n",
    "\n",
    "\\begin{equation*}\n",
    "Cost = \\sum_{x=1}^{n}(guess_{i} - y_{i})^{2}\n",
    "\\end{equation*}\n",
    "\n",
    "This is the total error for the current model, ie for the current m and b value sthat describe this line\n",
    "\n",
    "Our goal is to minimize this Cost/Loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use gradient descent to find minm of a function\n",
    "\n",
    "See the curve in the diag\n",
    "\n",
    "![](./data/img/diag8.png)\n",
    "\n",
    "We compute slope of the curve at a particular pt\n",
    "\n",
    "By computing the slope we know that we need to go down in order to get towards the minm\n",
    "\n",
    "As we go down slope becomes less extreme, so we dont need to go much further down anymore\n",
    "\n",
    "In our code we had:\n",
    "\n",
    "``` javascript\n",
    "\n",
    "function gradientDescent(){\n",
    "\n",
    "    var learning_rate = 0.05;\n",
    "\n",
    "    for(var i = 0; i < data.length; ++i){\n",
    "        var x = data[i].x;\n",
    "        var y = data[i].y;\n",
    "\n",
    "        // make a guess\n",
    "        var guess = m * x + b;\n",
    "\n",
    "        //find error\n",
    "        var error = y - guess;\n",
    "\n",
    "        // tune m & b\n",
    "        m = m + (error * x) * learning_rate;\n",
    "        b = b + (error * learning_rate);\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "We tweaked values of m and b for every data pt\n",
    "\n",
    "So we want to know how we can change values of m and b in y = mx+b in order to min error\n",
    "\n",
    "\\begin{equation*}\n",
    "m = m + \\Delta m\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "b = b + \\Delta b\n",
    "\\end{equation*}\n",
    "\n",
    "So somehow we need to find derivative of the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Derivitive\n",
    "\n",
    "We are calling the Cost function J\n",
    "\n",
    "And let \n",
    "\\begin{equation*}\n",
    "error = guess - y\n",
    "\\end{equation*}\n",
    "\n",
    "Also we wont consider the summation sign\n",
    "This has to do with the stochastic vs batch GD. We are basically looking at each error one at a time (stochastic GD) so we dont need the summation\n",
    "\n",
    "\\begin{equation*}\n",
    "J_{m,b} = error^{2}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Now we want to find partial derivative of J wrt m. This is basically **how the cost function changes with m. This is what will tell us tell us how to change the value of m for the next iteration**:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial m} = \\frac{\\partial J}{\\partial (error)} \\times \\frac{\\partial (error)}{\\partial m}\\rightarrow eqn1\n",
    "\\end{equation*}\n",
    "\n",
    "Similarly we want to find partial derivative of J wrt b. This is basically **how the cost function changes with b. This is what will tell us tell us how to change the value of b for the next iteration**:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial (error)} \\times \\frac{\\partial (error)}{\\partial b}\\rightarrow eqn2\n",
    "\\end{equation*}\n",
    "\n",
    "From eqn 1:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial m} = \\frac{\\partial J}{\\partial (error)} \\times \\frac{\\partial (error)}{\\partial m} = 2 \\times error \\times \\frac{\\partial (error)}{\\partial m}\n",
    "\\end{equation*}\n",
    "\n",
    "We know\n",
    "\n",
    "\\begin{equation*}\n",
    "guess = mx + b\n",
    "\\end{equation*}\n",
    "\n",
    "So \n",
    "\n",
    "\\begin{equation*}\n",
    "error = guess - y = mx + b - y\n",
    "\\end{equation*}\n",
    "\n",
    "So\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial (error)}{\\partial m} = x\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Thus\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial m} = 2 \\times error \\times x = error \\times x\n",
    "\\end{equation*}\n",
    "\n",
    "We ignore the const term as we take into account learning rate as the const term\n",
    "\n",
    "So basically\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Delta m = error \\times x \\times learningRate\n",
    "\\end{equation*}\n",
    "\n",
    "This is what we coded\n",
    "\n",
    "---\n",
    "\n",
    "Similarly we want to find partial derivative of J wrt b:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial (error)} \\times \\frac{\\partial (error)}{\\partial b}\\rightarrow eqn2\n",
    "\\end{equation*}\n",
    "\n",
    "From eqn 2:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{\\partial J}{\\partial (error)} \\times \\frac{\\partial (error)}{\\partial b} = 2 \\times error \\times \\frac{\\partial (error)}{\\partial b}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "So \n",
    "\n",
    "\\begin{equation*}\n",
    "error = guess - y = mx + b - y\n",
    "\\end{equation*}\n",
    "\n",
    "So\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial (error)}{\\partial b} = 1\n",
    "\\end{equation*}\n",
    "\n",
    "So basically\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Delta b = error \\times learningRate\n",
    "\\end{equation*}\n",
    "\n",
    "** In Neural Nw also the wt changes based on ip x error x learning_rate** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
